@article{anDesignAugmentedReality2018,
  title = {Design of {{Augmented Reality Head-up Display System Based}} on {{Image Semantic Segmentation}}},
  author = {An, Zhe and Xu, Xiping and Yang, Jinhua and Qiao, Yang and Liu, Yang},
  year = {2018},
  month = sep,
  journal = {Acta Optica Sinica},
  volume = {38},
  number = {7},
  pages = {0710004},
  issn = {0253-2239},
  doi = {10.3788/AOS201838.0710004},
  urldate = {2023-09-26},
  abstract = {Researching (High Level Discipline Journal Cluster English Platform), previously known as CLP Publishing (the English version of Chinese Optics Journal, 2019) was launched in April, 2021, which provides the platform for publishing world-class journals independently...},
  langid = {english},
  keywords = {material[r]}
}

@article{asgaritaghanakiDeepSemanticSegmentation2021,
  title = {Deep Semantic Segmentation of Natural and Medical Images: A Review},
  shorttitle = {Deep Semantic Segmentation of Natural and Medical Images},
  author = {Asgari Taghanaki, Saeid and Abhishek, Kumar and Cohen, Joseph Paul and {Cohen-Adad}, Julien and Hamarneh, Ghassan},
  year = {2021},
  month = jan,
  journal = {Artificial Intelligence Review},
  volume = {54},
  number = {1},
  pages = {137--178},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09854-1},
  urldate = {2023-09-14},
  abstract = {The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods~and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image segmentation.},
  langid = {english},
  keywords = {semantic segmentation},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2021_Asgari_Taghanaki_et.al_Deep_semantic_segmentation_of_natural_and_medical_images.pdf}
}

@misc{badrinarayananSegNetDeepConvolutional2016,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  year = {2016},
  month = oct,
  number = {arXiv:1511.00561},
  eprint = {1511.00561},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.00561},
  urldate = {2023-09-27},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2016_Badrinarayanan_et.al_SegNet.pdf;C\:\\Users\\yitao\\Zotero\\storage\\288FCS3J\\1511.html}
}

@inproceedings{boseInsectImageSemantic2023,
  title = {Insect {{Image Semantic Segmentation}} and~{{Identification Using UNET}} and~{{DeepLab V3}}+},
  booktitle = {{{ICT Infrastructure}} and {{Computing}}},
  author = {Bose, Kunal and Shubham, Kumar and Tiwari, Vivek and Patel, Kuldip Singh},
  editor = {Tuba, Milan and Akashe, Shyam and Joshi, Amit},
  year = {2023},
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  pages = {703--711},
  publisher = {{Springer Nature}},
  address = {{Singapore}},
  doi = {10.1007/978-981-19-5331-6_71},
  abstract = {Semantic image segmentation, which has become one of the most important applications in image processing and computer vision, has been used in a wide range of fields. This study aimed to construct a novel large-scale soybean insect dataset in unconstrained environment. It includes 2558 images of three species and their ground truth. Firstly, this paper summarizes the dataset development which includes reformatting, annotation, bounding box, masking, and splitting dataset. Furthermore, two deep learning methods (UNET and DeepLab V3+) have been applied for semantic segmentation of insect. Later, it compared the predicted labels of image segmentation models with respect to its ground truth in terms of IoU, loss, accuracy, precision, recall, and \$\$F\_1\$\$-score. The results are a witness in favor of the adopted feature engineering methods, where both the methods have demonstrated competitive performance, however, DeepLab V3+ slightly outperforms.},
  isbn = {978-981-19533-1-6},
  langid = {english},
  keywords = {SELECTED},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2023_Bose_et.al_Insect_Image_Semantic_Segmentation_and_Identification_Using_UNET_and_DeepLab_V3+2.pdf}
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom},
  year = {2020},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-ofthe-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous nonsparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Brown_et.al_Language_Models_are_Few-Shot_Learners.pdf}
}

@inproceedings{bucherZeroShotSemanticSegmentation2019,
  title = {Zero-{{Shot Semantic Segmentation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bucher, Maxime and VU, Tuan-Hung and Cord, Matthieu and P{\'e}rez, Patrick},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-07-02},
  abstract = {Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual  segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called generalized zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.},
  keywords = {SELECTED},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2019_Bucher_et.al_Zero-Shot_Semantic_Segmentation.pdf}
}

@misc{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  year = {2021},
  month = may,
  number = {arXiv:2104.14294},
  eprint = {2104.14294},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.14294},
  urldate = {2023-10-22},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Caron_et.al_Emerging_Properties_in_Self-Supervised_Vision_Transformers.pdf;C\:\\Users\\yitao\\Zotero\\storage\\55YW7DUT\\2104.html}
}

@inproceedings{caruanaEmpiricalComparisonSupervised2006,
  title = {An Empirical Comparison of Supervised Learning Algorithms},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Caruana, Rich and {Niculescu-Mizil}, Alexandru},
  year = {2006},
  month = jun,
  series = {{{ICML}} '06},
  pages = {161--168},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143865},
  urldate = {2023-09-18},
  abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
  isbn = {978-1-59593-383-6},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\2006_Caruana_Niculescu-Mizil_An_empirical_comparison_of_supervised_learning_algorithms.pdf}
}

@misc{chenDeepLabSemanticImage2016,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  shorttitle = {{{DeepLab}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2016},
  month = jun,
  journal = {arXiv.org},
  urldate = {2023-09-27},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  howpublished = {https://arxiv.org/abs/1606.00915v2},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2016_Chen_et.al_DeepLab.pdf}
}

@misc{chenEmpiricalStudyTraining2021,
  title = {An {{Empirical Study}} of {{Training Self-Supervised Vision Transformers}}},
  author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
  year = {2021},
  month = aug,
  number = {arXiv:2104.02057},
  eprint = {2104.02057},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.02057},
  urldate = {2023-10-22},
  abstract = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Chen_et.al_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers.pdf;C\:\\Users\\yitao\\Zotero\\storage\\8N25RUVM\\2104.html}
}

@misc{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2018},
  month = feb,
  journal = {arXiv.org},
  urldate = {2023-09-27},
  abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\textbackslash\% and 82.1\textbackslash\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \textbackslash url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
  howpublished = {https://arxiv.org/abs/1802.02611v3},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2018_Chen_et.al_Encoder-Decoder_with_Atrous_Separable_Convolution_for_Semantic_Image.pdf}
}

@misc{chenRethinkingAtrousConvolution2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  month = jun,
  journal = {arXiv.org},
  urldate = {2023-09-27},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  howpublished = {https://arxiv.org/abs/1706.05587v3},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2017_Chen_et.al_Rethinking_Atrous_Convolution_for_Semantic_Image_Segmentation.pdf}
}

@article{chenReviewImageClassification2021,
  title = {Review of {{Image Classification Algorithms Based}} on {{Convolutional Neural Networks}}},
  author = {Chen, Leiyu and Li, Shaobo and Bai, Qiang and Yang, Jing and Jiang, Sanlong and Miao, Yanming},
  year = {2021},
  month = jan,
  journal = {Remote Sensing},
  volume = {13},
  number = {22},
  pages = {4712},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2072-4292},
  doi = {10.3390/rs13224712},
  urldate = {2023-09-27},
  abstract = {Image classification has always been a hot research direction in the world, and the emergence of deep learning has promoted the development of this field. Convolutional neural networks (CNNs) have gradually become the mainstream algorithm for image classification since 2012, and the CNN architecture applied to other visual recognition tasks (such as object detection, object localization, and semantic segmentation) is generally derived from the network architecture in image classification. In the wake of these successes, CNN-based methods have emerged in remote sensing image scene classification and achieved advanced classification accuracy. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art (SOAT) network architectures. Along the way, we analyze (1) the basic structure of artificial neural networks (ANNs) and the basic network layers of CNNs, (2) the classic predecessor network models, (3) the recent SOAT network algorithms, (4) comprehensive comparison of various image classification methods mentioned in this article. Finally, we have also summarized the main analysis and discussion in this article, as well as introduce some of the current trends.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2021_Chen_et.al_Review_of_Image_Classification_Algorithms_Based_on_Convolutional_Neural_Networks.pdf}
}

@misc{chenSemanticImageSegmentation2014,
  title = {Semantic {{Image Segmentation}} with {{Deep Convolutional Nets}} and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2014},
  month = dec,
  journal = {arXiv.org},
  urldate = {2023-09-27},
  abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  howpublished = {https://arxiv.org/abs/1412.7062v4},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2014_Chen_et.al_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_and_Fully_Connected.pdf}
}

@misc{chiaburuExplanationsHumansHumans2023,
  title = {Explanations from {{Humans}} for {{Humans}}: {{The Potential}} of {{Transparent AI}} for {{Trust Calibration}} and {{Didactics}}},
  shorttitle = {Explanations from {{Humans}} for {{Humans}}},
  author = {Chiaburu, Teodor},
  year = {2023},
  month = aug,
  urldate = {2023-09-29},
  abstract = {XAI Experiments on an Annotated Dataset of Wild Bee Images}
}

@misc{chiaburuMLMethodsBiodiversity2023,
  title = {Towards {{ML Methods}} for {{Biodiversity}}: {{A Novel Wild Bee Dataset}} and {{Evaluations}} of {{XAI Methods}} for {{ML-Assisted Rare Species Annotations}}},
  shorttitle = {Towards {{ML Methods}} for {{Biodiversity}}},
  author = {Chiaburu, Teodor},
  year = {2023},
  month = feb,
  urldate = {2023-05-24},
  abstract = {XAI Experiments on an Annotated Dataset of Wild Bee Images},
  annotation = {titleTranslation: 朝着生物多样性的机器学习方法：一种新颖的野生蜜蜂数据集及用于机器学习辅助珍稀物种注释的可解释人工智能（XAI）方法的评估}
}

@misc{choPiCIEUnsupervisedSemantic2021a,
  title = {{{PiCIE}}: {{Unsupervised Semantic Segmentation}} Using {{Invariance}} and {{Equivariance}} in {{Clustering}}},
  shorttitle = {{{PiCIE}}},
  author = {Cho, Jang Hyun and Mall, Utkarsh and Bala, Kavita and Hariharan, Bharath},
  year = {2021},
  month = mar,
  number = {arXiv:2103.17070},
  eprint = {2103.17070},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.17070},
  urldate = {2023-10-22},
  abstract = {We present a new framework for semantic segmentation without annotations via clustering. Off-the-shelf clustering methods are limited to curated, single-label, and object-centric images yet real-world data are dominantly uncurated, multi-label, and scene-centric. We extend clustering from images to pixels and assign separate cluster membership to different instances within each image. However, solely relying on pixel-wise feature similarity fails to learn high-level semantic concepts and overfits to low-level visual cues. We propose a method to incorporate geometric consistency as an inductive bias to learn invariance and equivariance for photometric and geometric variations. With our novel learning objective, our framework can learn high-level semantic concepts. Our method, PiCIE (Pixel-level feature Clustering using Invariance and Equivariance), is the first method capable of segmenting both things and stuff categories without any hyperparameter tuning or task-specific pre-processing. Our method largely outperforms existing baselines on COCO and Cityscapes with +17.5 Acc. and +4.5 mIoU. We show that PiCIE gives a better initialization for standard supervised training. The code is available at https://github.com/janghyuncho/PiCIE.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Cho_et.al_PiCIE.pdf;C\:\\Users\\yitao\\Zotero\\storage\\XGSLIZVH\\2103.html}
}

@misc{COCOCommonObjects,
  title = {{{COCO}} - {{Common Objects}} in {{Context}}},
  urldate = {2023-05-24},
  howpublished = {https://cocodataset.org/\#home},
  file = {C:\Users\yitao\Zotero\storage\S6W2AQ99\cocodataset.org.html}
}

@misc{csurkaUnsupervisedDomainAdaptation2021,
  title = {Unsupervised {{Domain Adaptation}} for {{Semantic Image Segmentation}}: A {{Comprehensive Survey}}},
  shorttitle = {Unsupervised {{Domain Adaptation}} for {{Semantic Image Segmentation}}},
  author = {Csurka, Gabriela and Volpi, Riccardo and Chidlovskii, Boris},
  year = {2021},
  month = dec,
  journal = {arXiv.org},
  urldate = {2023-09-14},
  abstract = {Semantic segmentation plays a fundamental role in a broad variety of computer vision applications, providing key information for the global understanding of an image. Yet, the state-of-the-art models rely on large amount of annotated samples, which are more expensive to obtain than in tasks such as image classification. Since unlabelled data is instead significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation reached a broad success within the semantic segmentation community. This survey is an effort to summarize five years of this incredibly rapidly growing field, which embraces the importance of semantic segmentation itself and a critical need of adapting segmentation models to new environments. We present the most important semantic segmentation methods; we provide a comprehensive survey on domain adaptation techniques for semantic segmentation; we unveil newer trends such as multi-domain learning, domain generalization, test-time adaptation or source-free domain adaptation; we conclude this survey by describing datasets and benchmarks most widely used in semantic segmentation research. We hope that this survey will provide researchers across academia and industry with a comprehensive reference guide and will help them in fostering new research directions in the field.},
  howpublished = {https://arxiv.org/abs/2112.03241v1},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2021_Csurka_et.al_Unsupervised_Domain_Adaptation_for_Semantic_Image_Segmentation.pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-09-27},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2019_Devlin_et.al_BERT.pdf;C\:\\Users\\yitao\\Zotero\\storage\\JYGAMM8C\\1810.html}
}

@misc{doerschUnsupervisedVisualRepresentation2016,
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  year = {2016},
  month = jan,
  number = {arXiv:1505.05192},
  eprint = {1505.05192},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1505.05192},
  urldate = {2023-10-22},
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2016_Doersch_et.al_Unsupervised_Visual_Representation_Learning_by_Context_Prediction.pdf;C\:\\Users\\yitao\\Zotero\\storage\\MD2K4F5W\\1505.html}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2023-09-27},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Dosovitskiy_et.al_An_Image_is_Worth_16x16_Words.pdf;C\:\\Users\\yitao\\Zotero\\storage\\WWWVKQ35\\2010.html}
}

@article{fiedlerAlgebraicConnectivityGraphs1973,
  title = {Algebraic Connectivity of Graphs},
  author = {Fiedler, Miroslav},
  year = {1973},
  journal = {Czechoslovak Mathematical Journal},
  volume = {23},
  number = {2},
  pages = {298--305},
  issn = {0011-4642, 1572-9141},
  doi = {10.21136/CMJ.1973.101168},
  urldate = {2023-10-23},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Zotero\storage\ZI6L98RL\Fiedler - 1973 - Algebraic connectivity of graphs.pdf}
}

@misc{FullArticleSurvey,
  title = {Full Article: {{A Survey}} on {{Deep Learning-based Architectures}} for {{Semantic Segmentation}} on {{2D Images}}},
  urldate = {2023-09-14},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/08839514.2022.2032924},
  keywords = {semantic segmentation}
}

@article{gaoLargescaleUnsupervisedSemantic2023,
  title = {Large-Scale {{Unsupervised Semantic Segmentation}}},
  author = {Gao, Shanghua and Li, Zhong-Yu and Yang, Ming-Hsuan and Cheng, Ming-Ming and Han, Junwei and Torr, Philip},
  year = {2023},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {6},
  eprint = {2106.03149},
  primaryclass = {cs},
  pages = {7457--7476},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3218275},
  urldate = {2023-09-14},
  abstract = {Empowered by large datasets, e.g., ImageNet, unsupervised learning on large-scale data has enabled significant advances for classification tasks. However, whether the large-scale unsupervised semantic segmentation can be achieved remains unknown. There are two major challenges: i) we need a large-scale benchmark for assessing algorithms; ii) we need to develop methods to simultaneously learn category and shape representation in an unsupervised manner. In this work, we propose a new problem of large-scale unsupervised semantic segmentation (LUSS) with a newly created benchmark dataset to help the research progress. Building on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 50k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly/fully supervised methods accordingly, identifying the challenges and possible directions of LUSS. The benchmark and source code is publicly available at https://github.com/LUSSeg.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Gao_et.al_Large-scale_Unsupervised_Semantic_Segmentation.pdf;C\:\\Users\\yitao\\Zotero\\storage\\3LT6XCCD\\2106.html}
}

@misc{garcia-garciaReviewDeepLearning2017,
  title = {A {{Review}} on {{Deep Learning Techniques Applied}} to {{Semantic Segmentation}}},
  author = {{Garcia-Garcia}, Alberto and {Orts-Escolano}, Sergio and Oprea, Sergiu and {Villena-Martinez}, Victor and {Garcia-Rodriguez}, Jose},
  year = {2017},
  month = apr,
  number = {arXiv:1704.06857},
  eprint = {1704.06857},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.06857},
  urldate = {2023-09-26},
  abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2017_Garcia-Garcia_et.al_A_Review_on_Deep_Learning_Techniques_Applied_to_Semantic_Segmentation.pdf;C\:\\Users\\yitao\\Zotero\\storage\\S5BFWCEA\\1704.html}
}

@misc{gidarisUnsupervisedRepresentationLearning2018,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  year = {2018},
  month = mar,
  number = {arXiv:1803.07728},
  eprint = {1803.07728},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.07728},
  urldate = {2023-10-22},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2018_Gidaris_et.al_Unsupervised_Representation_Learning_by_Predicting_Image_Rotations.pdf;C\:\\Users\\yitao\\Zotero\\storage\\2RUB6JNC\\1803.html}
}

@article{guoReviewSemanticSegmentation2018,
  title = {A Review of Semantic Segmentation Using Deep Neural Networks},
  author = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S.},
  year = {2018},
  month = jun,
  journal = {International Journal of Multimedia Information Retrieval},
  volume = {7},
  number = {2},
  pages = {87--93},
  issn = {2192-662X},
  doi = {10.1007/s13735-017-0141-z},
  urldate = {2023-09-18},
  abstract = {During the long history of computer vision, one of the grand challenges has been semantic segmentation which is the ability to segment an unknown image into different parts and objects (e.g., beach, ocean, sun, dog, swimmer). Furthermore, segmentation is even deeper than object recognition because recognition is not necessary for segmentation. Specifically, humans can perform image segmentation without even knowing what the objects are (for example, in satellite imagery or medical X-ray scans, there may be several objects which are unknown, but they can still be segmented within the image typically for further investigation). Performing segmentation without knowing the exact identity of all objects in the scene is an important part of our visual understanding process which can give us a powerful model to understand the world and also be used to improve or augment existing computer vision techniques. Herein this work, we review the field of semantic segmentation as pertaining to deep convolutional neural networks. We provide comprehensive coverage of the top approaches and summarize the strengths, weaknesses and major challenges.},
  langid = {english},
  keywords = {semantic segmentation},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2018_Guo_et.al_A_review_of_semantic_segmentation_using_deep_neural_networks.pdf}
}

@misc{hamiltonUnsupervisedSemanticSegmentation2022,
  title = {Unsupervised {{Semantic Segmentation}} by {{Distilling Feature Correspondences}}},
  author = {Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T.},
  year = {2022},
  month = mar,
  number = {arXiv:2203.08414},
  eprint = {2203.08414},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.08414},
  urldate = {2023-09-25},
  abstract = {Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (\$\textbackslash textbf\{S\}\$elf-supervised \$\textbackslash textbf\{T\}\$ransformer with \$\textbackslash textbf\{E\}\$nergy-based \$\textbackslash textbf\{G\}\$raph \$\textbackslash textbf\{O\}\$ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (\$\textbackslash textbf\{+14 mIoU\}\$) and Cityscapes (\$\textbackslash textbf\{+9 mIoU\}\$) semantic segmentation challenges.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2022_Hamilton_et.al_Unsupervised_Semantic_Segmentation_by_Distilling_Feature_Correspondences.pdf;C\:\\Users\\yitao\\Zotero\\storage\\M9V3KRP2\\2203.html}
}

@incollection{hancerArtificialBeeColony2020,
  title = {Artificial {{Bee Colony}}: {{Theory}}, {{Literature Review}}, and {{Application}} in {{Image Segmentation}}},
  shorttitle = {Artificial {{Bee Colony}}},
  booktitle = {Recent {{Advances}} on {{Memetic Algorithms}} and Its {{Applications}} in {{Image Processing}}},
  author = {Hancer, Emrah},
  editor = {Hemanth, D. Jude and Kumar, B. Vinoth and Manavalan, G. R. Karpagam},
  year = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {47--67},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-15-1362-6_3},
  urldate = {2023-09-21},
  abstract = {The artificial bee colony (ABC) is one of the most well-regarded swarm intelligence-based algorithms in the literature of evolutionary computation techniques. This algorithm mimics the foraging behaviors of bees in the hive. Due to its well-designed search ability and dependency on fewer control parameters, ABC has extensive usage in various fields. One of the remarkable areas where ABC has been successfully implemented is the image segmentation. Image segmentation is the process of dividing a digital image into multiple segments. The overall goal of image segmentation is to extract meaningful information from an image. Although considerable attempts have been made to develop image segmentation techniques using ABC, it is not possible to find any work in the literature that particularly seeks to reflect the profile of ABC-based image segmentation techniques. This chapter first tries to describe ABC-based image segmentation techniques from the fundamental concepts of segmentation such as clustering, thresholding, and edge detection. The chapter also applies ABC algorithm to a challenging task in image segmentation. It is observed that ABC can accurately segment the regions of an image.},
  isbn = {9789811513626},
  langid = {english}
}

@article{hanSurveyVisionTransformer2023,
  title = {A {{Survey}} on {{Vision Transformer}}},
  author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
  year = {2023},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {1},
  pages = {87--110},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3152247},
  urldate = {2023-09-27},
  abstract = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Han_et.al_A_Survey_on_Vision_Transformer.pdf;C\:\\Users\\yitao\\Zotero\\storage\\33WR9X99\\9716741.html}
}

@article{haoBriefSurveySemantic2020,
  title = {A {{Brief Survey}} on {{Semantic Segmentation}} with {{Deep Learning}}},
  author = {Hao, Shijie and Zhou, Yuan and Guo, Yanrong},
  year = {2020},
  month = sep,
  journal = {Neurocomputing},
  volume = {406},
  pages = {302--321},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.11.118},
  urldate = {2023-09-14},
  abstract = {Semantic segmentation is a challenging task in computer vision. In recent years, the performance of semantic segmentation has been greatly improved by using deep learning techniques. A large number of novel methods have been proposed. This paper aims to provide a brief review of research efforts on deep-learning-based semantic segmentation methods. We categorize the related research according to its supervision level, i.e., fully-supervised methods, weakly-supervised methods and semi-supervised methods. We also discuss the common challenges of the current research, and present several valuable growing research points in this field. This survey is expected to familiarize readers with the progress and challenges of semantic segmentation research in the deep learning era.},
  keywords = {semantic segmentation},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Hao_et.al_A_Brief_Survey_on_Semantic_Segmentation_with_Deep_Learning.pdf}
}

@misc{INaturalist,
  title = {{iNaturalist}},
  journal = {iNaturalist},
  urldate = {2023-09-29},
  abstract = {Xylocopa violacea 的观察},
  howpublished = {https://www.inaturalist.org/observations?photo\_license=CC-BY-NC\&quality\_grade=research\&subview=table\&taxon\_id=124145},
  langid = {chinese},
  file = {C:\Users\yitao\Zotero\storage\WKERDKP7\observations.html}
}

@misc{islamRecentAdvancesVision2022,
  title = {Recent {{Advances}} in {{Vision Transformer}}: {{A Survey}} and {{Outlook}} of {{Recent Work}}},
  shorttitle = {Recent {{Advances}} in {{Vision Transformer}}},
  author = {Islam, Khawar},
  year = {2022},
  month = aug,
  number = {arXiv:2203.01536},
  eprint = {2203.01536},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.01536},
  urldate = {2023-09-27},
  abstract = {Vision Transformers (ViTs) are becoming more popular and dominating technique for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a demanding technique in computer vision, ViTs have been successfully solved various vision problems while focusing on long-range relationships. In this paper, we begin by introducing the fundamental concepts and background of the self-attention mechanism. Next, we provide a comprehensive overview of recent top-performing ViT methods describing in terms of strength and weakness, computational cost as well as training and testing dataset. We thoroughly compare the performance of various ViT algorithms and most representative CNN methods on popular benchmark datasets. Finally, we explore some limitations with insightful observations and provide further research direction. The project page along with the collections of papers are available at https://github.com/khawar512/ViT-Survey},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2022_Islam_Recent_Advances_in_Vision_Transformer.pdf;C\:\\Users\\yitao\\Zotero\\storage\\FSNBYY4S\\2203.html}
}

@article{jiangMedicalImageSemantic2018,
  title = {Medical Image Semantic Segmentation Based on Deep Learning},
  author = {Jiang, Feng and Grigorev, Aleksei and Rho, Seungmin and Tian, Zhihong and Fu, YunSheng and Jifara, Worku and Adil, Khan and Liu, Shaohui},
  year = {2018},
  month = mar,
  journal = {Neural Computing and Applications},
  volume = {29},
  number = {5},
  pages = {1257--1265},
  issn = {1433-3058},
  doi = {10.1007/s00521-017-3158-6},
  urldate = {2023-09-26},
  abstract = {The image semantic segmentation has been extensively studying. The modern methods rely on the deep convolutional neural networks, which can be trained to address this problem. A few years ago networks require the huge dataset to be trained. However, the recent advances in deep learning allow training networks on the small datasets, which is a critical issue for medical images, since the hospitals and research organizations usually do not provide the huge amount of data. In this paper, we address medical image semantic segmentation problem by applying the modern CNN model. Moreover, the recent achievements in deep learning allow processing the whole image per time by applying concepts of the fully convolutional neural network. Our qualitative and quantitate experiment results demonstrated that modern CNN can successfully tackle the medical image semantic segmentation problem.},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2018_Jiang_et.al_Medical_image_semantic_segmentation_based_on_deep_learning.pdf}
}

@article{jiangningMethodsInsectImage2011,
  title = {{Methods of insect image segmentation and their application.}},
  author = {JiangNing, Wang and LiQiang, Ji},
  year = {2011},
  journal = {Acta Entomologica Sinica},
  volume = {54},
  number = {2},
  pages = {211--217},
  publisher = {{Editorial Office of Acta Zootaxonomica Sinica}},
  issn = {0454-6296},
  urldate = {2023-09-21},
  abstract = {Automatic identification on insect images is a rapid identification method, and image segmentation is a key process in insect image automatic identification. Based on searching and sorting the recent references on insect image segmentation, we found that the research on insect image segmentation are increasing rapidly. With the development of image processing technology, all kinds of segmentation...},
  langid = {chinese}
}

@misc{jiInvariantInformationClustering2019,
  title = {Invariant {{Information Clustering}} for {{Unsupervised Image Classification}} and {{Segmentation}}},
  author = {Ji, Xu and Henriques, Joao F. and Vedaldi, Andrea},
  year = {2019},
  month = aug,
  number = {arXiv:1807.06653},
  eprint = {1807.06653},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.06653},
  urldate = {2023-10-22},
  abstract = {We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8\% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90\% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2019_Ji_et.al_Invariant_Information_Clustering_for_Unsupervised_Image_Classification_and.pdf;C\:\\Users\\yitao\\Zotero\\storage\\6FXAVE4L\\1807.html}
}

@article{khanTransformersVisionSurvey2022,
  title = {Transformers in {{Vision}}: {{A Survey}}},
  shorttitle = {Transformers in {{Vision}}},
  author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2022},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  pages = {200:1--200:41},
  issn = {0360-0300},
  doi = {10.1145/3505244},
  urldate = {2023-09-27},
  abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2022_Khan_et.al_Transformers_in_Vision.pdf}
}

@misc{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2023},
  month = apr,
  number = {arXiv:2304.02643},
  eprint = {2304.02643},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.02643},
  urldate = {2023-07-02},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Kirillov_et.al_Segment_Anything.pdf;C\:\\Users\\yitao\\Zotero\\storage\\FLHFKTDW\\2304.html}
}

@article{linSurveyTransformers2022,
  title = {A Survey of Transformers},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  year = {2022},
  month = jan,
  journal = {AI Open},
  volume = {3},
  pages = {111--132},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2022.10.001},
  urldate = {2023-09-27},
  abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2022_Lin_et.al_A_survey_of_transformers.pdf}
}

@misc{liReferringImageMatting2023,
  title = {Referring {{Image Matting}}},
  author = {Li, Jizhizi and Zhang, Jing and Tao, Dacheng},
  year = {2023},
  urldate = {2023-07-13},
  abstract = {Different from conventional image matting, which either requires user-defined scribbles/trimap to extract a specific foreground object or directly extracts all the foreground objects in the image indiscriminately, we introduce a new task named Referring Image Matting (RIM) in this paper, which aims to extract the meticulous alpha matte of the specific object that best matches the given natural language description, thus enabling a more natural and simpler instruction for image matting. First, we establish a large-scale challenging dataset RefMatte by designing a comprehensive image composition and expression generation engine to automatically produce high-quality images along with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we construct a real-world test set with 100 high-resolution natural images and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods. Furthermore, we present a novel baseline method CLIPMat for RIM, including a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Extensive experiments on RefMatte in both keyword and expression settings validate the superiority of CLIPMat over representative methods. We hope this work could provide novel insights into image matting and encourage more followup studies. The dataset, code and models are available at https://github.com/JizhiziLi/RIM.},
  howpublished = {https://arxiv.org/abs/2206.05149},
  annotation = {abstractTranslation:},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Li_et.al_Referring_Image_Matting.pdf;C\:\\Users\\yitao\\Zotero\\storage\\K4W962K6\\2206.html}
}

@misc{liTransformerBasedVisualSegmentation2023,
  title = {Transformer-{{Based Visual Segmentation}}: {{A Survey}}},
  shorttitle = {Transformer-{{Based Visual Segmentation}}},
  author = {Li, Xiangtai and Ding, Henghui and Zhang, Wenwei and Yuan, Haobo and Pang, Jiangmiao and Cheng, Guangliang and Chen, Kai and Liu, Ziwei and Loy, Chen Change},
  year = {2023},
  month = jun,
  number = {arXiv:2304.09854},
  eprint = {2304.09854},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.09854},
  urldate = {2023-09-26},
  abstract = {Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Li_et.al_Transformer-Based_Visual_Segmentation.pdf;C\:\\Users\\yitao\\Zotero\\storage\\5S4JDPWQ\\2304.html}
}

@article{liuPlantDiseasesPests2021,
  title = {Plant Diseases and Pests Detection Based on Deep Learning: A Review},
  shorttitle = {Plant Diseases and Pests Detection Based on Deep Learning},
  author = {Liu, Jun and Wang, Xuewei},
  year = {2021},
  month = feb,
  journal = {Plant Methods},
  volume = {17},
  number = {1},
  pages = {22},
  issn = {1746-4811},
  doi = {10.1186/s13007-021-00722-9},
  urldate = {2023-09-21},
  abstract = {Plant diseases and pests are important factors determining the yield and quality of plants. Plant diseases and pests identification can be carried out by means of digital image processing. In recent years, deep learning has made breakthroughs in the field of digital image processing, far superior to traditional methods. How to use deep learning technology to study plant diseases and pests identification has become a research issue of great concern to researchers. This review provides a definition of plant diseases and pests detection problem, puts forward a comparison with traditional plant diseases and pests detection methods. According to the difference of network structure, this study outlines the research on plant diseases and pests detection based on deep learning in recent years from three aspects of classification network, detection network and segmentation network, and the advantages and disadvantages of each method are summarized. Common datasets are introduced, and the performance of existing studies is compared. On this basis, this study discusses possible challenges in practical applications of plant diseases and pests detection based on deep learning. In addition, possible solutions and research ideas are proposed for the challenges, and several suggestions are given. Finally, this study gives the analysis and prospect of the future trend of plant diseases and pests detection based on deep learning.},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2021_Liu_Wang_Plant_diseases_and_pests_detection_based_on_deep_learning.pdf}
}

@article{liuSurveyVisualTransformers2023,
  title = {A {{Survey}} of {{Visual Transformers}}},
  author = {Liu, Yang and Zhang, Yao and Wang, Yixin and Hou, Feng and Yuan, Jin and Tian, Jiang and Zhang, Yang and Shi, Zhongchao and Fan, Jianping and He, Zhiqiang},
  year = {2023},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--21},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3227717},
  urldate = {2023-09-27},
  abstract = {Transformer, an attention-based encoder\textendash decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern convolution neural networks (CNNs). In this survey, we have reviewed over 100 of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, two promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Liu_et.al_A_Survey_of_Visual_Transformers.pdf;C\:\\Users\\yitao\\Zotero\\storage\\LJSX5J28\\10088164.html}
}

@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.14030},
  urldate = {2023-09-20},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Liu_et.al_Swin_Transformer.pdf;C\:\\Users\\yitao\\Zotero\\storage\\VWJKZ2BK\\2103.html}
}

@inproceedings{longFullyConvolutionalNetworks2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = jun,
  pages = {3431--3440},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  keywords = {semantic segmentation},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2015_Long_et.al_Fully_convolutional_networks_for_semantic_segmentation.pdf;C\:\\Users\\yitao\\Zotero\\storage\\TQTUVNG5\\7298965.html}
}

@misc{luddeckeImageSegmentationUsing2022,
  title = {Image {{Segmentation Using Text}} and {{Image Prompts}}},
  author = {L{\"u}ddecke, Timo and Ecker, Alexander S.},
  year = {2022},
  month = mar,
  number = {arXiv:2112.10003},
  eprint = {2112.10003},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10003},
  urldate = {2023-05-16},
  abstract = {Image segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system that can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated. Finally, we find our system to adapt well to generalized queries involving affordances or properties. Code is available at https://eckerlab.org/code/clipseg.},
  archiveprefix = {arxiv},
  keywords = {SELECTED},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2022_Lüddecke_Ecker_Image_Segmentation_Using_Text_and_Image_Prompts.pdf;C\:\\Users\\yitao\\Zotero\\storage\\624BHVRB\\2112.html;C\:\\Users\\yitao\\Zotero\\storage\\LCWB6BSY\\clipseg-zero-shot.html}
}

@inproceedings{luoMultiTaskCollaborativeNetwork2020,
  title = {Multi-{{Task Collaborative Network}} for {{Joint Referring Expression Comprehension}} and {{Segmentation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Luo, Gen and Zhou, Yiyi and Sun, Xiaoshuai and Cao, Liujuan and Wu, Chenglin and Deng, Cheng and Ji, Rongrong},
  year = {2020},
  month = jun,
  pages = {10031--10040},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01005},
  urldate = {2023-09-29},
  abstract = {Referring expression comprehension (REC) and segmentation (RES) are two highly-related tasks, which both aim at identifying the referent according to a natural language expression. In this paper, we propose a novel Multi-task Collaborative Network (MCN)1 to achieve a joint learning of REC and RES for the first time. In MCN, RES can help REC to achieve better language-vision alignment, while REC can help RES to better locate the referent. In addition, we address a key challenge in this multi-task setup, i.e., the prediction conflict, with two innovative designs namely, Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM enables REC and RES to focus on similar visual regions by maximizing the consistency energy between two tasks. ASNLS supresses the response of unrelated regions in RES based on the prediction of REC. To validate our model, we conduct extensive experiments on three benchmark datasets of REC and RES, i.e., RefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant performance gains of MCN over all existing methods, i.e., up to +7.13\% for REC and +11.50\% for RES over SOTA, which well confirm the validity of our model for joint REC and RES learning.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Luo_et.al_Multi-Task_Collaborative_Network_for_Joint_Referring_Expression_Comprehension.pdf}
}

@misc{MeasuringPerformanceConfusion2019,
  title = {Measuring {{Performance}}: {{The Confusion Matrix}}},
  shorttitle = {Measuring {{Performance}}},
  year = {2019},
  month = feb,
  journal = {Glass Box},
  urldate = {2023-10-23},
  abstract = {Confusion matrices are calculated using the predictions of a model on a data set. By looking at a confusion matrix, you can gain a better understanding of the strengths and weaknesses of your model\ldots},
  langid = {english},
  file = {C:\Users\yitao\Zotero\storage\IRMCXZ8A\measuring-performance-the-confusion-matrix.html}
}

@article{melas-kyriaziDeepSpectralMethods2022,
  title = {Deep {{Spectral Methods}}: {{A Surprisingly Strong Baseline}} for {{Unsupervised Semantic Segmentation}} and {{Localization}}},
  shorttitle = {Deep {{Spectral Methods}}},
  author = {{Melas-Kyriazi}, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2205.07839},
  urldate = {2023-05-09},
  abstract = {Unsupervised localization and segmentation are long-standing computer vision challenges that involve decomposing an image into semantically-meaningful segments without any labeled data. These tasks are particularly interesting in an unsupervised setting due to the difficulty and cost of obtaining dense image annotations, but existing unsupervised approaches struggle with complex scenes containing multiple objects. Differently from existing methods, which are purely based on deep learning, we take inspiration from traditional spectral segmentation methods by reframing image decomposition as a graph partitioning problem. Specifically, we examine the eigenvectors of the Laplacian of a feature affinity matrix from self-supervised networks. We find that these eigenvectors already decompose an image into meaningful segments, and can be readily used to localize objects in a scene. Furthermore, by clustering the features associated with these segments across a dataset, we can obtain well-delineated, nameable regions, i.e. semantic segmentations. Experiments on complex datasets (Pascal VOC, MS-COCO) demonstrate that our simple spectral method outperforms the state-of-the-art in unsupervised localization and segmentation by a significant margin. Furthermore, our method can be readily used for a variety of complex image editing tasks, such as background removal and compositing.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {SELECTED,semantic segmentation},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2022_Melas-Kyriazi_et.al_Deep_Spectral_Methods.pdf}
}

@article{minaeeImageSegmentationUsing2022,
  title = {Image {{Segmentation Using Deep Learning}}: {{A Survey}}},
  shorttitle = {Image {{Segmentation Using Deep Learning}}},
  author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {7},
  pages = {3523--3542},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3059968},
  abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2022_Minaee_et.al_Image_Segmentation_Using_Deep_Learning.pdf;C\:\\Users\\yitao\\Zotero\\storage\\BI4RKF67\\9356353.html}
}

@inproceedings{mohd-isaImageSegmentationMeliponine2019,
  title = {Image {{Segmentation}} of {{Meliponine Bee}} Using {{Faster R-CNN}}},
  booktitle = {2019 {{Third World Conference}} on {{Smart Trends}} in {{Systems Security}} and {{Sustainablity}} ({{WorldS4}})},
  author = {{Mohd-Isa}, Wan-Noorshahida and Nizam, Afif and Ali, Aziah},
  year = {2019},
  month = jul,
  pages = {235--238},
  doi = {10.1109/WorldS4.2019.8904005},
  abstract = {There has been an increasing interest by local Malaysian towards beekeeping of Meliponine, which is a tribe of stingless bee. When it comes to advice on beekeeping, these locals (mostly novice) depends on inputs from only a handful of key experts (mostly aging). This paper presents part of a project that aims to develop a visual-based expert system, which may help in beekeeping tasks. One of the first processes in such system is image segmentation. Being small in size, the segmentation of a Meliponine in its natural surrounding is difficult to be accurately detected, more so identified. Of recent, the Convolutional Neural Network (CNN) has been making tremendous progress in object detection due to its high accuracy and fast execution. In this paper, the Faster R-CNN, an object detection method, which uses the CNN core module is implemented for segmentation of Meliponine image from its background. We present results of this implementation on our data set of 400 image frames from videos collected in a local Meliponine farm in Malaysia. The accuracy result at around 74\% looks promising for further exploration.},
  keywords = {material[r],SELECTED},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2019_Mohd-Isa_et.al_Image_Segmentation_of_Meliponine_Bee_using_Faster_R-CNN.pdf;C\:\\Users\\yitao\\Zotero\\storage\\I8349B23\\8904005.html}
}

@article{moReviewStateoftheartTechnologies2022,
  title = {Review the State-of-the-Art Technologies of Semantic Segmentation Based on Deep Learning},
  author = {Mo, Yujian and Wu, Yan and Yang, Xinneng and Liu, Feilin and Liao, Yujun},
  year = {2022},
  month = jul,
  journal = {Neurocomputing},
  volume = {493},
  pages = {626--646},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.01.005},
  urldate = {2023-09-14},
  abstract = {The goal of semantic segmentation is to segment the input image according to semantic information and predict the semantic category of each pixel from a given label set. With the gradual intellectualization of modern life, more and more applications need to infer relevant semantic information from images for subsequent processing, such as augmented reality, autonomous driving, video surveillance, etc. This paper reviews the state-of-the-art technologies of semantic segmentation based on deep learning. Because semantic segmentation requires a large number of pixel-level annotations, in order to reduce the fine-grained requirements of annotation and reduce the economic and time cost of manual annotation, this paper studies the works on weakly-supervised semantic segmentation. In order to enhance the generalization ability and robustness of the segmentation model, this paper investigates the works on domain adaptation in semantic segmentation. Many types of sensors are usually equipped in some practical applications, such as autonomous driving and medical image analysis. In order to mine the association between multi-modal data and improve the accuracy of the segmentation model, this paper investigates the works based on multi-modal data fusion semantic segmentation. The real-time performance of the model needs to be considered in practical application. This paper analyzes the key factors affecting the real-time performance of the segmentation model and investigates the works on real-time semantic segmentation. Finally, this paper summarizes the challenges and promising research directions of semantic segmentation tasks based on deep learning.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2022_Mo_et.al_Review_the_state-of-the-art_technologies_of_semantic_segmentation_based_on_deep.pdf;C\:\\Users\\yitao\\Zotero\\storage\\KUEJP55N\\S0925231222000054.html}
}

@misc{norooziUnsupervisedLearningVisual2017,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  year = {2017},
  month = aug,
  number = {arXiv:1603.09246},
  eprint = {1603.09246},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.09246},
  urldate = {2023-10-22},
  abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2017_Noroozi_Favaro_Unsupervised_Learning_of_Visual_Representations_by_Solving_Jigsaw_Puzzles.pdf;C\:\\Users\\yitao\\Zotero\\storage\\5HHIMQCK\\1603.html}
}

@article{olaodeUnsupervisedClassificationImages2014,
  title = {Unsupervised {{Classification}} of {{Images}}: {{A Review}}},
  shorttitle = {Unsupervised {{Classification}} of {{Images}}},
  author = {Olaode, Abass and Naghdy, Golshah and Todd, Catherine},
  year = {2014},
  month = sep,
  journal = {International Journal of Image Processing},
  volume = {8},
  pages = {2014--325},
  abstract = {Unsupervised image classification is the process by which each image in a dataset is identified to be a member of one of the inherent categories present in the image collection without the use of labelled training samples. Unsupervised categorisation of images relies on unsupervised machine learning algorithms for its implementation. This paper identifies clustering algorithms and dimension reduction algorithms as the two main classes of unsupervised machine learning algorithms needed in unsupervised image categorisation, and then reviews how these algorithms are used in some notable implementation of unsupervised image classification algorithms.},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2014_Olaode_et.al_Unsupervised_Classification_of_Images.pdf}
}

@misc{osheaIntroductionConvolutionalNeural2015,
  title = {An {{Introduction}} to {{Convolutional Neural Networks}}},
  author = {O'Shea, Keiron and Nash, Ryan},
  year = {2015},
  month = dec,
  number = {arXiv:1511.08458},
  eprint = {1511.08458},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.08458},
  urldate = {2023-09-21},
  abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2015_O'Shea_Nash_An_Introduction_to_Convolutional_Neural_Networks.pdf;C\:\\Users\\yitao\\Zotero\\storage\\5LCAVS3T\\1511.html}
}

@article{OtsuMethod2023,
  title = {Otsu's Method},
  year = {2023},
  month = aug,
  journal = {Wikipedia},
  urldate = {2023-10-27},
  abstract = {In computer vision and image processing, Otsu's method, named after Nobuyuki Otsu (大津展之, \=Otsu Nobuyuki), is used to perform automatic image thresholding. In the simplest form, the algorithm returns a single intensity threshold that separate pixels into two classes, foreground and background. This threshold is determined by minimizing intra-class intensity variance, or equivalently, by maximizing inter-class variance. Otsu's method is a one-dimensional discrete analogue of Fisher's Discriminant Analysis, is related to Jenks optimization method, and is equivalent to a globally optimal k-means performed on the intensity histogram. The extension to multi-level thresholding was described in the original paper, and computationally efficient implementations have since been proposed.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1169237407},
  file = {C:\Users\yitao\Zotero\storage\IUU6UVQK\Otsu's_method.html}
}

@article{otsuThresholdSelectionMethod1979,
  title = {A {{Threshold Selection Method}} from {{Gray-Level Histograms}}},
  author = {Otsu, Nobuyuki},
  year = {1979},
  month = jan,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {9},
  number = {1},
  pages = {62--66},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1979.4310076},
  urldate = {2023-10-19},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\1979_Otsu_A_Threshold_Selection_Method_from_Gray-Level_Histograms.pdf}
}

@article{panCrossViewSemanticSegmentation2020,
  title = {Cross-{{View Semantic Segmentation}} for {{Sensing Surroundings}}},
  author = {Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei},
  year = {2020},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {3},
  pages = {4867--4873},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3004325},
  urldate = {2023-09-26},
  abstract = {Sensing surroundings plays a crucial role in human spatial perception, as it extracts the spatial configuration of objects as well as the free space from the observations. To facilitate the robot perception with such a surrounding sensing capability, we introduce a novel visual task called Cross-view Semantic Segmentation as well as a framework named View Parsing Network (VPN) to address it. In the cross-view semantic segmentation task, the agent is trained to parse the first-view observations into a top-down-view semantic map indicating the spatial location of all the objects at pixel-level. The main issue of this task is that we lack the real-world annotations of top-down-view data. To mitigate this, we train the VPN in 3D graphics environment and utilize the domain adaptation technique to transfer it to handle real-world data. We evaluate our VPN on both synthetic and real-world agents. The experimental results show that our model can effectively make use of the information from different views and multi-modalities to understanding spatial information. Our further experiment on a LoCoBot robot shows that our model enables the surrounding sensing capability from 2D image input. Code and demo videos can be found at https://view-parsing-network.github.io.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2020_Pan_et.al_Cross-View_Semantic_Segmentation_for_Sensing_Surroundings.pdf;C\:\\Users\\yitao\\Zotero\\storage\\AASHLCD2\\9123682.html}
}

@inproceedings{papandreouWeaklyandSemiSupervisedLearning2015,
  title = {Weakly-and {{Semi-Supervised Learning}} of a {{Deep Convolutional Network}} for {{Semantic Image Segmentation}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Papandreou, George and Chen, Liang-Chieh and Murphy, Kevin P. and Yuille, Alan L.},
  year = {2015},
  month = dec,
  pages = {1742--1750},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.203},
  urldate = {2023-09-25},
  abstract = {Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2015_Papandreou_et.al_Weakly-and_Semi-Supervised_Learning_of_a_Deep_Convolutional_Network_for.pdf;C\:\\Users\\yitao\\Zotero\\storage\\BARR8FPZ\\7410560.html}
}

@misc{pathakContextEncodersFeature2016,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  year = {2016},
  month = nov,
  number = {arXiv:1604.07379},
  eprint = {1604.07379},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.07379},
  urldate = {2023-10-22},
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2016_Pathak_et.al_Context_Encoders.pdf;C\:\\Users\\yitao\\Zotero\\storage\\RWE65592\\1604.html}
}

@misc{pelaez-vegasSurveySemiSupervisedSemantic2023,
  title = {A {{Survey}} on {{Semi-Supervised Semantic Segmentation}}},
  author = {{Pel{\'a}ez-Vegas}, Adrian and Mesejo, Pablo and Luengo, Juli{\'a}n},
  year = {2023},
  month = feb,
  number = {arXiv:2302.09899},
  eprint = {2302.09899},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.09899},
  urldate = {2023-09-20},
  abstract = {Semantic segmentation is one of the most challenging tasks in computer vision. However, in many applications, a frequent obstacle is the lack of labeled images, due to the high cost of pixel-level labeling. In this scenario, it makes sense to approach the problem from a semi-supervised point of view, where both labeled and unlabeled images are exploited. In recent years this line of research has gained much interest and many approaches have been published in this direction. Therefore, the main objective of this study is to provide an overview of the current state of the art in semi-supervised semantic segmentation, offering an updated taxonomy of all existing methods to date. This is complemented by an experimentation with a variety of models representing all the categories of the taxonomy on the most widely used becnhmark datasets in the literature, and a final discussion on the results obtained, the challenges and the most promising lines of future research.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Peláez-Vegas_et.al_A_Survey_on_Semi-Supervised_Semantic_Segmentation.pdf;C\:\\Users\\yitao\\Zotero\\storage\\G6SAGBD5\\2302.html}
}

@misc{perezFiLMVisualReasoning2017,
  title = {{{FiLM}}: {{Visual Reasoning}} with a {{General Conditioning Layer}}},
  shorttitle = {{{FiLM}}},
  author = {Perez, Ethan and Strub, Florian and {de Vries}, Harm and Dumoulin, Vincent and Courville, Aaron},
  year = {2017},
  month = dec,
  number = {arXiv:1709.07871},
  eprint = {1709.07871},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-27},
  abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning \textemdash{} answering image-related questions which require a multi-step, high-level process \textemdash{} a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-theart error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {C:\Users\yitao\Zotero\storage\J8ZVS9E2\Perez 等 - 2017 - FiLM Visual Reasoning with a General Conditioning.pdf}
}

@misc{qinFreeSegUnifiedUniversal2023,
  title = {{{FreeSeg}}: {{Unified}}, {{Universal}} and {{Open-Vocabulary Image Segmentation}}},
  shorttitle = {{{FreeSeg}}},
  author = {Qin, Jie and Wu, Jie and Yan, Pengxiang and Li, Ming and Yuxi, Ren and Xiao, Xuefeng and Wang, Yitong and Wang, Rui and Wen, Shilei and Pan, Xin and Wang, Xingang},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17225},
  eprint = {2303.17225},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.17225},
  urldate = {2023-09-20},
  abstract = {Recently, open-vocabulary learning has emerged to accomplish segmentation for arbitrary categories of text-based descriptions, which popularizes the segmentation system to more general-purpose application scenarios. However, existing methods devote to designing specialized architectures or parameters for specific segmentation tasks. These customized design paradigms lead to fragmentation between various segmentation tasks, thus hindering the uniformity of segmentation models. Hence in this paper, we propose FreeSeg, a generic framework to accomplish Unified, Universal and Open-Vocabulary Image Segmentation. FreeSeg optimizes an all-in-one network via one-shot training and employs the same architecture and parameters to handle diverse segmentation tasks seamlessly in the inference procedure. Additionally, adaptive prompt learning facilitates the unified model to capture task-aware and category-sensitive concepts, improving model robustness in multi-task and varied scenarios. Extensive experimental results demonstrate that FreeSeg establishes new state-of-the-art results in performance and generalization on three segmentation tasks, which outperforms the best task-specific architectures by a large margin: 5.5\% mIoU on semantic segmentation, 17.6\% mAP on instance segmentation, 20.1\% PQ on panoptic segmentation for the unseen class on COCO.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Qin_et.al_FreeSeg.pdf;C\:\\Users\\yitao\\Zotero\\storage\\3SKS3MRB\\2303.html}
}

@article{qiuPretrainedModelsNatural2020,
  title = {Pre-Trained Models for Natural Language Processing: {{A}} Survey},
  shorttitle = {Pre-Trained Models for Natural Language Processing},
  author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
  year = {2020},
  month = oct,
  journal = {Science China Technological Sciences},
  volume = {63},
  number = {10},
  pages = {1872--1897},
  issn = {1869-1900},
  doi = {10.1007/s11431-020-1647-3},
  urldate = {2023-09-27},
  abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Qiu_et.al_Pre-trained_models_for_natural_language_processing.pdf}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2018_Radford_et.al_Improving_Language_Understanding_by_Generative_Pre-Training.pdf}
}

@article{radfordLanguageModelsAre2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2019_Radford_et.al_Language_Models_are_Unsupervised_Multitask_Learners.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2023-07-03},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Radford_et.al_Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf;C\:\\Users\\yitao\\Zotero\\storage\\SDZ6LN28\\2103.html}
}

@misc{raffelExploringLimitsTransfer2019,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2019},
  month = oct,
  journal = {arXiv.org},
  urldate = {2023-09-28},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  howpublished = {https://arxiv.org/abs/1910.10683v4},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2019_Raffel_et.al_Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text.pdf}
}

@misc{RemoteSensingFree,
  title = {Remote {{Sensing}} | {{Free Full-Text}} | {{Review}} of {{Image Classification Algorithms Based}} on {{Convolutional Neural Networks}}},
  urldate = {2023-09-27},
  howpublished = {https://www.mdpi.com/2072-4292/13/22/4712},
  file = {C:\Users\yitao\Zotero\storage\64C8WQC9\4712.html}
}

@misc{renCrossDomainSelfsupervisedMultitask2017,
  title = {Cross-{{Domain Self-supervised Multi-task Feature Learning}} Using {{Synthetic Imagery}}},
  author = {Ren, Zhongzheng and Lee, Yong Jae},
  year = {2017},
  month = nov,
  number = {arXiv:1711.09082},
  eprint = {1711.09082},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.09082},
  urldate = {2023-10-22},
  abstract = {In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multi-task learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classification and 2012 detection.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2017_Ren_Lee_Cross-Domain_Self-supervised_Multi-task_Feature_Learning_using_Synthetic_Imagery.pdf;C\:\\Users\\yitao\\Zotero\\storage\\YW6XBT82\\1711.html}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-09-27},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  howpublished = {https://arxiv.org/abs/1505.04597v1},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2015_Ronneberger_et.al_U-Net.pdf}
}

@article{schmarjeSurveySemiSelf2021,
  title = {A {{Survey}} on {{Semi-}}, {{Self-}} and {{Unsupervised Learning}} for {{Image Classification}}},
  author = {Schmarje, Lars and Santarossa, Monty and Schr{\"o}der, Simon-Martin and Koch, Reinhard},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {82146--82168},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3084358},
  urldate = {2023-09-27},
  abstract = {While deep learning strategies achieve outstanding results in computer vision tasks, one issue remains: The current strategies rely heavily on a huge amount of labeled data. In many real-world problems, it is not feasible to create such an amount of labeled training data. Therefore, it is common to incorporate unlabeled data into the training process to reach equal results with fewer labels. Due to a lot of concurrent research, it is difficult to keep track of recent developments. In this survey, we provide an overview of often used ideas and methods in image classification with fewer labels. We compare 34 methods in detail based on their performance and their commonly used ideas rather than a fine-grained taxonomy. In our analysis, we identify three major trends that lead to future research opportunities. 1. State-of-the-art methods are scalable to real-world applications in theory but issues like class imbalance, robustness, or fuzzy labels are not considered. 2. The degree of supervision which is needed to achieve comparable results to the usage of all labels is decreasing and therefore methods need to be extended to settings with a variable number of classes. 3. All methods share some common ideas but we identify clusters of methods that do not share many ideas. We show that combining ideas from different clusters can lead to better performance.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Schmarje_et.al_A_Survey_on_Semi-,_Self-_and_Unsupervised_Learning_for_Image_Classification.pdf;C\:\\Users\\yitao\\Zotero\\storage\\WT57JZF6\\9442775.html}
}

@misc{SemantischeSegmentierung,
  title = {{Semantische Segmentierung}},
  urldate = {2023-10-23},
  abstract = {Erfahren Sie, wie Sie mithilfe von Deep Learning eine semantische Segmentierung in MATLAB durchf\"uhren k\"onnen. Die Ressourcen umfassen Videos, Beispiele und Dokumentationen f\"ur semantische Segmentierungen, neuronale Faltungsnetze, Bildklassifizierung und andere Themen.},
  howpublished = {https://de.mathworks.com/solutions/image-video-processing/semantic-segmentation.html},
  langid = {ngerman},
  file = {C:\Users\yitao\Zotero\storage\MPYZL7BW\semantic-segmentation.html}
}

@misc{shabanOneShotLearningSemantic2017,
  title = {One-{{Shot Learning}} for {{Semantic Segmentation}}},
  author = {Shaban, Amirreza and Bansal, Shray and Liu, Zhen and Essa, Irfan and Boots, Byron},
  year = {2017},
  month = sep,
  journal = {arXiv.org},
  urldate = {2023-09-29},
  abstract = {Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25\% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.},
  howpublished = {https://arxiv.org/abs/1709.03410v1},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2017_Shaban_et.al_One-Shot_Learning_for_Semantic_Segmentation.pdf}
}

@article{shiNormalizedCutsImage2000,
  title = {Normalized Cuts and Image Segmentation},
  author = {Shi, Jianbo and Malik, J.},
  year = {2000},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {8},
  pages = {888--905},
  issn = {1939-3539},
  doi = {10.1109/34.868688},
  urldate = {2023-10-23},
  abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2000_Shi_Malik_Normalized_cuts_and_image_segmentation.pdf;C\:\\Users\\yitao\\Zotero\\storage\\VDRFAFF6\\868688.html}
}

@misc{simeoniLocalizingObjectsSelfSupervised2021,
  title = {Localizing {{Objects}} with {{Self-Supervised Transformers}} and No {{Labels}}},
  author = {Sim{\'e}oni, Oriane and Puy, Gilles and Vo, Huy V. and Roburin, Simon and Gidaris, Spyros and Bursuc, Andrei and P{\'e}rez, Patrick and Marlet, Renaud and Ponce, Jean},
  year = {2021},
  month = sep,
  number = {arXiv:2109.14279},
  eprint = {2109.14279},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.14279},
  urldate = {2023-10-22},
  abstract = {Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. The code to reproduce our results can be found at https://github.com/valeoai/LOST.},
  archiveprefix = {arxiv},
  keywords = {SELECTED},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Siméoni_et.al_Localizing_Objects_with_Self-Supervised_Transformers_and_no_Labels.pdf;C\:\\Users\\yitao\\Zotero\\storage\\EMNRAEN4\\2109.html}
}

@misc{strudelSegmenterTransformerSemantic2021a,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  year = {2021},
  month = sep,
  number = {arXiv:2105.05633},
  eprint = {2105.05633},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.05633},
  urldate = {2023-09-25},
  abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Strudel_et.al_Segmenter2.pdf;C\:\\Users\\yitao\\Zotero\\storage\\PRZ2A8CZ\\2105.html}
}

@article{toldoUnsupervisedDomainAdaptation2020,
  title = {Unsupervised {{Domain Adaptation}} in {{Semantic Segmentation}}: {{A Review}}},
  shorttitle = {Unsupervised {{Domain Adaptation}} in {{Semantic Segmentation}}},
  author = {Toldo, Marco and Maracani, Andrea and Michieli, Umberto and Zanuttigh, Pietro},
  year = {2020},
  month = jun,
  journal = {Technologies},
  volume = {8},
  number = {2},
  pages = {35},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2227-7080},
  doi = {10.3390/technologies8020035},
  urldate = {2023-09-14},
  abstract = {The aim of this paper is to give an overview of the recent advancements in the Unsupervised Domain Adaptation (UDA) of deep networks for semantic segmentation. This task is attracting a wide interest since semantic segmentation models require a huge amount of labeled data and the lack of data fitting specific requirements is the main limitation in the deployment of these techniques. This field has been recently explored and has rapidly grown with a large number of ad-hoc approaches. This motivates us to build a comprehensive overview of the proposed methodologies and to provide a clear categorization. In this paper, we start by introducing the problem, its formulation and the various scenarios that can be considered. Then, we introduce the different levels at which adaptation strategies may be applied: namely, at the input (image) level, at the internal features representation and at the output level. Furthermore, we present a detailed overview of the literature in the field, dividing previous methods based on the following (non mutually exclusive) categories: adversarial learning, generative-based, analysis of the classifier discrepancies, self-teaching, entropy minimization, curriculum learning and multi-task learning. Novel research directions are also briefly introduced to give a hint of interesting open problems in the field. Finally, a comparison of the performance of the various methods in the widely used autonomous driving scenario is presented.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Toldo_et.al_Unsupervised_Domain_Adaptation_in_Semantic_Segmentation.pdf}
}

@article{tremlSpeedingSemanticSegmentation2016,
  title = {Speeding up {{Semantic Segmentation}} for {{Autonomous Driving}}},
  author = {Treml, Michael and {Arjona-Medina}, Jos{\'e} and Unterthiner, Thomas and Durgesh, Rupesh and Friedmann, Felix and Schuberth, Peter and Mayr, Andreas and Heusel, Martin and Hofmarcher, Markus and Widrich, Michael and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2016},
  month = oct,
  urldate = {2023-09-26},
  abstract = {Deep learning has considerably improved semantic image segmentation. However, its high accuracy is traded against larger computational costs which makes it unsuit- able for embedded devices in self-driving cars. We propose a novel deep network architecture for image segmentation that keeps the high accuracy while being efficient enough for embedded devices. The architecture consists of ELU activation functions, a SqueezeNet-like encoder, followed by parallel dilated convolutions, and a decoder with SharpMask-like refinement modules. On the Cityscapes dataset, the new network achieves higher segmentation accuracy than other networks that are tailored to embedded devices. Simultaneously the frame-rate is still sufficiently high for the deployment in autonomous vehicles.},
  langid = {english},
  keywords = {material[r]},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2016_Treml_et.al_Speeding_up_Semantic_Segmentation_for_Autonomous_Driving.pdf}
}

@article{ulkuSurveyDeepLearningbased2022,
  title = {A {{Survey}} on {{Deep Learning-based Architectures}} for {{Semantic Segmentation}} on {{2D Images}}},
  author = {Ulku, Irem and Akag{\"u}nd{\"u}z, Erdem},
  year = {2022},
  month = dec,
  journal = {Applied Artificial Intelligence},
  volume = {36},
  number = {1},
  pages = {2032924},
  publisher = {{Taylor \& Francis}},
  issn = {0883-9514},
  doi = {10.1080/08839514.2022.2032924},
  urldate = {2023-09-14},
  abstract = {Semantic segmentation is the pixel-wise labeling of an image. Boosted by the extraordinary ability of convolutional neural networks (CNN) in creating semantic, high-level and hierarchical image features; several deep learning-based 2D semantic segmentation approaches have been proposed within the last decade. In this survey, we mainly focus on the recent scientific developments in semantic segmentation, specifically on deep learning-based methods using 2D images. We started with an analysis of the public image sets and leaderboards for 2D semantic segmentation, with an overview of the techniques employed in performance evaluation. In examining the evolution of the field, we chronologically categorized the approaches into three main periods, namely pre-and early deep learning era, the fully convolutional era, and the post-FCN era. We technically analyzed the solutions put forward in terms of solving the fundamental problems of the field, such as fine-grained localization and scale invariance. Before drawing our conclusions, we present a table of methods from all mentioned eras, with a summary of each approach that explains their contribution to the field. We conclude the survey by discussing the current challenges of the field and to what extent they have been solved.},
  keywords = {semantic segmentation},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2022_Ulku_Akagündüz_A_Survey_on_Deep_Learning-based_Architectures_for_Semantic_Segmentation_on_2D.pdf}
}

@misc{vangansbekeSCANLearningClassify2020,
  title = {{{SCAN}}: {{Learning}} to {{Classify Images}} without {{Labels}}},
  shorttitle = {{{SCAN}}},
  author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Proesmans, Marc and Van Gool, Luc},
  year = {2020},
  month = jul,
  number = {arXiv:2005.12320},
  eprint = {2005.12320},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-12},
  abstract = {Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6\% on CIFAR10, +25.0\% on CIFAR100-20 and +21.3\% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any groundtruth annotations. The code is made publicly available here.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Van_Gansbeke_et.al_SCAN.pdf}
}

@misc{vangansbekeUnsupervisedSemanticSegmentation2021,
  title = {Unsupervised {{Semantic Segmentation}} by {{Contrasting Object Mask Proposals}}},
  author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Van Gool, Luc},
  year = {2021},
  month = aug,
  number = {arXiv:2102.06191},
  eprint = {2102.06191},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.06191},
  urldate = {2023-09-14},
  abstract = {Being able to learn dense semantic representations of images without supervision is an important problem in computer vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that considered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. In this paper, we make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a two-step framework that adopts a predetermined mid-level prior in a contrastive optimization objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the importance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner. Experimental evaluation shows that our method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means on PASCAL. Under the fully unsupervised setting, there is no precedent in solving the semantic segmentation task on such a challenging benchmark. Second, our representations can improve over strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The code is available.},
  archiveprefix = {arxiv},
  keywords = {material[r]},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Van_Gansbeke_et.al_Unsupervised_Semantic_Segmentation_by_Contrasting_Object_Mask_Proposals.pdf;C\:\\Users\\yitao\\Zotero\\storage\\BN3P78F8\\2102.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {material[r],SELECTED},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2017_Vaswani_et.al_Attention_is_All_you_Need.pdf}
}

@article{wangCognitiveVisionMethod2018,
  title = {A {{Cognitive Vision Method}} for {{Insect Pest Image Segmentation}}},
  author = {Wang, Zhibin and Wang, Kaiyi and Liu, Zhongqiang and Wang, Xiaofeng and Pan, Shouhui},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {6th {{IFAC Conference}} on {{Bio-Robotics BIOROBOTICS}} 2018},
  volume = {51},
  number = {17},
  pages = {85--89},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.08.066},
  urldate = {2023-09-21},
  abstract = {Segmentation of pests from crop leaves is the prerequisite step for pest intelligent diagnosis. To improve the accuracy and stability of segmentation, a cognitive segmentation approach to pest images is present in this paper. The method works in the follow way. First, a pest image is divided into blocks via an image block processing method. Second, an adaptive learning algorithm is used to accurately select the initial cluster centers. Third, preliminary segmentation results are achieved using K-means clustering. Finally, three digital morphological features of an ellipse are adopted to remove leaf veins. Segmentation experiments were performed on crop images of whiteflies. Compared with the conventional methods, the proposed cognitive segmentation method was accurate and robust for segmentation of whitefly images, and will provide a foundation for further identifying these pests.},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2018_Wang_et.al_A_Cognitive_Vision_Method_for_Insect_Pest_Image_Segmentation.pdf}
}

@misc{wuPhraseCutLanguagebasedImage2020,
  title = {{{PhraseCut}}: {{Language-based Image Segmentation}} in the {{Wild}}},
  shorttitle = {{{PhraseCut}}},
  author = {Wu, Chenyun and Lin, Zhe and Cohen, Scott and Bui, Trung and Maji, Subhransu},
  year = {2020},
  month = aug,
  number = {arXiv:2008.01187},
  eprint = {2008.01187},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2008.01187},
  urldate = {2023-10-13},
  abstract = {We consider the problem of segmenting image regions given a natural language phrase, and study it on a novel dataset of 77,262 images and 345,486 phrase-region pairs. Our dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated. Phrases in our dataset correspond to multiple regions and describe a large number of object and stuff categories as well as their attributes such as color, shape, parts, and relationships with other entities in the image. Our experiments show that the scale and diversity of concepts in our dataset poses significant challenges to the existing state-of-the-art. We systematically handle the long-tail nature of these concepts and present a modular approach to combine category, attribute, and relationship cues that outperforms existing approaches.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2020_Wu_et.al_PhraseCut.pdf;C\:\\Users\\yitao\\Zotero\\storage\\UPL6PSB8\\2008.html}
}

@misc{xieSegFormerSimpleEfficient2021,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  shorttitle = {{{SegFormer}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
  year = {2021},
  month = oct,
  number = {arXiv:2105.15203},
  eprint = {2105.15203},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.15203},
  urldate = {2023-09-26},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2021_Xie_et.al_SegFormer.pdf;C\:\\Users\\yitao\\Zotero\\storage\\2LN8A5AY\\2105.html}
}

@article{yuFESNetFrequencyEnhancedSaliency2023,
  title = {{{FESNet}}: {{Frequency-Enhanced Saliency Detection Network}} for {{Grain Pest Segmentation}}},
  shorttitle = {{{FESNet}}},
  author = {Yu, Junwei and Zhai, Fupin and Liu, Nan and Shen, Yi and Pan, Quan},
  year = {2023},
  month = feb,
  journal = {Insects},
  volume = {14},
  number = {2},
  pages = {99},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2075-4450},
  doi = {10.3390/insects14020099},
  urldate = {2023-10-25},
  abstract = {As insect infestation is the leading factor accounting for nutritive and economic losses in stored grains, it is important to detect the presence and number of insects for the sake of taking proper control measures. Inspired by the human visual attention mechanism, we propose a U-net-like frequency-enhanced saliency (FESNet) detection model, resulting in the pixelwise segmentation of grain pests. The frequency clues, as well as the spatial information, are leveraged to enhance the detection performance of small insects from the cluttered grain background. Firstly, we collect a dedicated dataset, GrainPest, with pixel-level annotation after analyzing the image attributes of the existing salient object detection datasets. Secondly, we design a FESNet with the discrete wavelet transformation (DWT) and the discrete cosine transformation (DCT), both involved in the traditional convolution layers. As current salient object detection models will reduce the spatial information with pooling operations in the sequence of encoding stages, a special branch of the discrete wavelet transformation (DWT) is connected to the higher stages to capture accurate spatial information for saliency detection. Then, we introduce the discrete cosine transform (DCT) into the backbone bottlenecks to enhance the channel attention with low-frequency information. Moreover, we also propose a new receptive field block (NRFB) to enlarge the receptive fields by aggregating three atrous convolution features. Finally, in the phase of decoding, we use the high-frequency information and aggregated features together to restore the saliency map. Extensive experiments and ablation studies on our dataset, GrainPest, and open dataset, Salient Objects in Clutter (SOC), demonstrate that the proposed model performs favorably against the state-of-the-art model.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2023_Yu_et.al_FESNet.pdf}
}

@article{zhangSGOneSimilarityGuidance2020,
  title = {{{SG-One}}: {{Similarity Guidance Network}} for {{One-Shot Semantic Segmentation}}},
  shorttitle = {{{SG-One}}},
  author = {Zhang, Xiaolin and Wei, Yunchao and Yang, Yi and Huang, Thomas S.},
  year = {2020},
  month = sep,
  journal = {IEEE Transactions on Cybernetics},
  volume = {50},
  number = {9},
  pages = {3855--3865},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.2992433},
  urldate = {2023-09-29},
  abstract = {One-shot image semantic segmentation poses a challenging task of recognizing the object regions from unseen categories with only one annotated example as supervision. In this article, we propose a simple yet effective similarity guidance network to tackle the one-shot (SG-One) segmentation problem. We aim at predicting the segmentation mask of a query image with the reference to one densely labeled support image of the same category. To obtain the robust representative feature of the support image, we first adopt a masked average pooling strategy for producing the guidance features by only taking the pixels belonging to the support image into account. We then leverage the cosine similarity to build the relationship between the guidance features and features of pixels from the query image. In this way, the possibilities embedded in the produced similarity maps can be adopted to guide the process of segmenting objects. Furthermore, our SG-One is a unified framework that can efficiently process both support and query images within one network and be learned in an end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In particular, our SG-One achieves the mIoU score of 46.3\%, surpassing the baseline methods.},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Zhang_et.al_SG-One.pdf}
}

@article{zhangSurveySemiWeakly2020,
  title = {A Survey of Semi- and Weakly Supervised Semantic Segmentation of Images},
  author = {Zhang, Man and Zhou, Yong and Zhao, Jiaqi and Man, Yiyun and Liu, Bing and Yao, Rui},
  year = {2020},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {6},
  pages = {4259--4288},
  issn = {1573-7462},
  doi = {10.1007/s10462-019-09792-7},
  urldate = {2023-09-14},
  abstract = {Image semantic segmentation is one of the most important tasks in the field of computer vision, and it has made great progress in many applications. Many fully supervised deep learning models are designed to implement complex semantic segmentation tasks and the experimental results are remarkable. However, the acquisition of pixel-level labels in fully supervised learning is time consuming and laborious, semi-supervised and weakly supervised learning is gradually replacing fully supervised learning, thus achieving good results at a lower cost. Based on the commonly used models such as convolutional neural networks, fully convolutional networks, generative adversarial networks, this paper focuses on the core methods and reviews the semi- and weakly supervised semantic segmentation models in recent years. In the following chapters, existing evaluations and data sets are summarized in details and the experimental results are analyzed according to the data set. The last part of the paper is an objective summary. In addition, it points out the possible direction of research and inspiring suggestions for future work.},
  langid = {english},
  keywords = {semantic segmentation},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2020_Zhang_et.al_A_survey_of_semi-_and_weakly_supervised_semantic_segmentation_of_images.pdf}
}

@inproceedings{zhaoDataAugmentationUsing2019,
  title = {Data {{Augmentation Using Learned Transformations}} for {{One-Shot Medical Image Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhao, Amy and Balakrishnan, Guha and Durand, Fredo and Guttag, John V. and Dalca, Adrian V.},
  year = {2019},
  pages = {8543--8553},
  urldate = {2023-09-29},
  file = {C:\Users\yitao\Nutstore\1\_BACKUPs\Zotero\pdfs\Project\bee\2019_Zhao_et.al_Data_Augmentation_Using_Learned_Transformations_for_One-Shot_Medical_Image.pdf}
}

@article{zhouBriefIntroductionWeakly2018a,
  title = {A Brief Introduction to Weakly Supervised Learning},
  author = {Zhou, Zhi-Hua},
  year = {2018},
  month = jan,
  journal = {National Science Review},
  volume = {5},
  number = {1},
  pages = {44--53},
  issn = {2095-5138},
  doi = {10.1093/nsr/nwx106},
  urldate = {2023-09-27},
  abstract = {Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2018_Zhou_A_brief_introduction_to_weakly_supervised_learning.pdf;C\:\\Users\\yitao\\Zotero\\storage\\9HLID2B2\\4093912.html}
}

@misc{ziwenAutoFocusFormerImageSegmentation2023,
  title = {{{AutoFocusFormer}}: {{Image Segmentation}} off the {{Grid}}},
  shorttitle = {{{AutoFocusFormer}}},
  author = {Ziwen, Chen and Patnaik, Kaushik and Zhai, Shuangfei and Wan, Alvin and Ren, Zhile and Schwing, Alex and Colburn, Alex and Fuxin, Li},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12406},
  eprint = {2304.12406},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12406},
  urldate = {2023-09-20},
  abstract = {Real world images often have highly imbalanced content density. Some areas are very uniform, e.g., large patches of blue sky, while other areas are scattered with many small objects. Yet, the commonly used successive grid downsampling strategy in convolutional deep networks treats all areas equally. Hence, small objects are represented in very few spatial locations, leading to worse results in tasks such as segmentation. Intuitively, retaining more pixels representing small objects during downsampling helps to preserve important information. To achieve this, we propose AutoFocusFormer (AFF), a local-attention transformer image recognition backbone, which performs adaptive downsampling by learning to retain the most important pixels for the task. Since adaptive downsampling generates a set of pixels irregularly distributed on the image plane, we abandon the classic grid structure. Instead, we develop a novel point-based local attention block, facilitated by a balanced clustering module and a learnable neighborhood merging module, which yields representations for our point-based versions of state-of-the-art segmentation heads. Experiments show that our AutoFocusFormer (AFF) improves significantly over baseline models of similar sizes.},
  archiveprefix = {arxiv},
  file = {C\:\\Users\\yitao\\Nutstore\\1\\_BACKUPs\\Zotero\\pdfs\\Project\\bee\\2023_Ziwen_et.al_AutoFocusFormer.pdf;C\:\\Users\\yitao\\Zotero\\storage\\WYZTZPDL\\2304.html}
}
