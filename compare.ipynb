{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from clipseg.models.clipseg import CLIPDensePredT\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gc\n",
    "import io\n",
    "from collections import namedtuple\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['clip_model.positional_embedding', 'clip_model.text_projection', 'clip_model.logit_scale', 'clip_model.visual.class_embedding', 'clip_model.visual.positional_embedding', 'clip_model.visual.proj', 'clip_model.visual.conv1.weight', 'clip_model.visual.ln_pre.weight', 'clip_model.visual.ln_pre.bias', 'clip_model.visual.transformer.resblocks.0.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.0.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.0.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.0.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.0.ln_1.weight', 'clip_model.visual.transformer.resblocks.0.ln_1.bias', 'clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.0.ln_2.weight', 'clip_model.visual.transformer.resblocks.0.ln_2.bias', 'clip_model.visual.transformer.resblocks.1.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.1.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.1.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.1.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.1.ln_1.weight', 'clip_model.visual.transformer.resblocks.1.ln_1.bias', 'clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.1.ln_2.weight', 'clip_model.visual.transformer.resblocks.1.ln_2.bias', 'clip_model.visual.transformer.resblocks.2.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.2.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.2.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.2.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.2.ln_1.weight', 'clip_model.visual.transformer.resblocks.2.ln_1.bias', 'clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.2.ln_2.weight', 'clip_model.visual.transformer.resblocks.2.ln_2.bias', 'clip_model.visual.transformer.resblocks.3.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.3.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.3.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.3.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.3.ln_1.weight', 'clip_model.visual.transformer.resblocks.3.ln_1.bias', 'clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.3.ln_2.weight', 'clip_model.visual.transformer.resblocks.3.ln_2.bias', 'clip_model.visual.transformer.resblocks.4.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.4.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.4.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.4.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.4.ln_1.weight', 'clip_model.visual.transformer.resblocks.4.ln_1.bias', 'clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.4.ln_2.weight', 'clip_model.visual.transformer.resblocks.4.ln_2.bias', 'clip_model.visual.transformer.resblocks.5.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.5.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.5.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.5.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.5.ln_1.weight', 'clip_model.visual.transformer.resblocks.5.ln_1.bias', 'clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.5.ln_2.weight', 'clip_model.visual.transformer.resblocks.5.ln_2.bias', 'clip_model.visual.transformer.resblocks.6.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.6.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.6.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.6.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.6.ln_1.weight', 'clip_model.visual.transformer.resblocks.6.ln_1.bias', 'clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.6.ln_2.weight', 'clip_model.visual.transformer.resblocks.6.ln_2.bias', 'clip_model.visual.transformer.resblocks.7.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.7.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.7.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.7.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.7.ln_1.weight', 'clip_model.visual.transformer.resblocks.7.ln_1.bias', 'clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.7.ln_2.weight', 'clip_model.visual.transformer.resblocks.7.ln_2.bias', 'clip_model.visual.transformer.resblocks.8.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.8.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.8.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.8.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.8.ln_1.weight', 'clip_model.visual.transformer.resblocks.8.ln_1.bias', 'clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.8.ln_2.weight', 'clip_model.visual.transformer.resblocks.8.ln_2.bias', 'clip_model.visual.transformer.resblocks.9.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.9.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.9.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.9.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.9.ln_1.weight', 'clip_model.visual.transformer.resblocks.9.ln_1.bias', 'clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.9.ln_2.weight', 'clip_model.visual.transformer.resblocks.9.ln_2.bias', 'clip_model.visual.transformer.resblocks.10.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.10.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.10.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.10.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.10.ln_1.weight', 'clip_model.visual.transformer.resblocks.10.ln_1.bias', 'clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.10.ln_2.weight', 'clip_model.visual.transformer.resblocks.10.ln_2.bias', 'clip_model.visual.transformer.resblocks.11.attn.in_proj_weight', 'clip_model.visual.transformer.resblocks.11.attn.in_proj_bias', 'clip_model.visual.transformer.resblocks.11.attn.out_proj.weight', 'clip_model.visual.transformer.resblocks.11.attn.out_proj.bias', 'clip_model.visual.transformer.resblocks.11.ln_1.weight', 'clip_model.visual.transformer.resblocks.11.ln_1.bias', 'clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'clip_model.visual.transformer.resblocks.11.ln_2.weight', 'clip_model.visual.transformer.resblocks.11.ln_2.bias', 'clip_model.visual.ln_post.weight', 'clip_model.visual.ln_post.bias', 'clip_model.transformer.resblocks.0.attn.in_proj_weight', 'clip_model.transformer.resblocks.0.attn.in_proj_bias', 'clip_model.transformer.resblocks.0.attn.out_proj.weight', 'clip_model.transformer.resblocks.0.attn.out_proj.bias', 'clip_model.transformer.resblocks.0.ln_1.weight', 'clip_model.transformer.resblocks.0.ln_1.bias', 'clip_model.transformer.resblocks.0.mlp.c_fc.weight', 'clip_model.transformer.resblocks.0.mlp.c_fc.bias', 'clip_model.transformer.resblocks.0.mlp.c_proj.weight', 'clip_model.transformer.resblocks.0.mlp.c_proj.bias', 'clip_model.transformer.resblocks.0.ln_2.weight', 'clip_model.transformer.resblocks.0.ln_2.bias', 'clip_model.transformer.resblocks.1.attn.in_proj_weight', 'clip_model.transformer.resblocks.1.attn.in_proj_bias', 'clip_model.transformer.resblocks.1.attn.out_proj.weight', 'clip_model.transformer.resblocks.1.attn.out_proj.bias', 'clip_model.transformer.resblocks.1.ln_1.weight', 'clip_model.transformer.resblocks.1.ln_1.bias', 'clip_model.transformer.resblocks.1.mlp.c_fc.weight', 'clip_model.transformer.resblocks.1.mlp.c_fc.bias', 'clip_model.transformer.resblocks.1.mlp.c_proj.weight', 'clip_model.transformer.resblocks.1.mlp.c_proj.bias', 'clip_model.transformer.resblocks.1.ln_2.weight', 'clip_model.transformer.resblocks.1.ln_2.bias', 'clip_model.transformer.resblocks.2.attn.in_proj_weight', 'clip_model.transformer.resblocks.2.attn.in_proj_bias', 'clip_model.transformer.resblocks.2.attn.out_proj.weight', 'clip_model.transformer.resblocks.2.attn.out_proj.bias', 'clip_model.transformer.resblocks.2.ln_1.weight', 'clip_model.transformer.resblocks.2.ln_1.bias', 'clip_model.transformer.resblocks.2.mlp.c_fc.weight', 'clip_model.transformer.resblocks.2.mlp.c_fc.bias', 'clip_model.transformer.resblocks.2.mlp.c_proj.weight', 'clip_model.transformer.resblocks.2.mlp.c_proj.bias', 'clip_model.transformer.resblocks.2.ln_2.weight', 'clip_model.transformer.resblocks.2.ln_2.bias', 'clip_model.transformer.resblocks.3.attn.in_proj_weight', 'clip_model.transformer.resblocks.3.attn.in_proj_bias', 'clip_model.transformer.resblocks.3.attn.out_proj.weight', 'clip_model.transformer.resblocks.3.attn.out_proj.bias', 'clip_model.transformer.resblocks.3.ln_1.weight', 'clip_model.transformer.resblocks.3.ln_1.bias', 'clip_model.transformer.resblocks.3.mlp.c_fc.weight', 'clip_model.transformer.resblocks.3.mlp.c_fc.bias', 'clip_model.transformer.resblocks.3.mlp.c_proj.weight', 'clip_model.transformer.resblocks.3.mlp.c_proj.bias', 'clip_model.transformer.resblocks.3.ln_2.weight', 'clip_model.transformer.resblocks.3.ln_2.bias', 'clip_model.transformer.resblocks.4.attn.in_proj_weight', 'clip_model.transformer.resblocks.4.attn.in_proj_bias', 'clip_model.transformer.resblocks.4.attn.out_proj.weight', 'clip_model.transformer.resblocks.4.attn.out_proj.bias', 'clip_model.transformer.resblocks.4.ln_1.weight', 'clip_model.transformer.resblocks.4.ln_1.bias', 'clip_model.transformer.resblocks.4.mlp.c_fc.weight', 'clip_model.transformer.resblocks.4.mlp.c_fc.bias', 'clip_model.transformer.resblocks.4.mlp.c_proj.weight', 'clip_model.transformer.resblocks.4.mlp.c_proj.bias', 'clip_model.transformer.resblocks.4.ln_2.weight', 'clip_model.transformer.resblocks.4.ln_2.bias', 'clip_model.transformer.resblocks.5.attn.in_proj_weight', 'clip_model.transformer.resblocks.5.attn.in_proj_bias', 'clip_model.transformer.resblocks.5.attn.out_proj.weight', 'clip_model.transformer.resblocks.5.attn.out_proj.bias', 'clip_model.transformer.resblocks.5.ln_1.weight', 'clip_model.transformer.resblocks.5.ln_1.bias', 'clip_model.transformer.resblocks.5.mlp.c_fc.weight', 'clip_model.transformer.resblocks.5.mlp.c_fc.bias', 'clip_model.transformer.resblocks.5.mlp.c_proj.weight', 'clip_model.transformer.resblocks.5.mlp.c_proj.bias', 'clip_model.transformer.resblocks.5.ln_2.weight', 'clip_model.transformer.resblocks.5.ln_2.bias', 'clip_model.transformer.resblocks.6.attn.in_proj_weight', 'clip_model.transformer.resblocks.6.attn.in_proj_bias', 'clip_model.transformer.resblocks.6.attn.out_proj.weight', 'clip_model.transformer.resblocks.6.attn.out_proj.bias', 'clip_model.transformer.resblocks.6.ln_1.weight', 'clip_model.transformer.resblocks.6.ln_1.bias', 'clip_model.transformer.resblocks.6.mlp.c_fc.weight', 'clip_model.transformer.resblocks.6.mlp.c_fc.bias', 'clip_model.transformer.resblocks.6.mlp.c_proj.weight', 'clip_model.transformer.resblocks.6.mlp.c_proj.bias', 'clip_model.transformer.resblocks.6.ln_2.weight', 'clip_model.transformer.resblocks.6.ln_2.bias', 'clip_model.transformer.resblocks.7.attn.in_proj_weight', 'clip_model.transformer.resblocks.7.attn.in_proj_bias', 'clip_model.transformer.resblocks.7.attn.out_proj.weight', 'clip_model.transformer.resblocks.7.attn.out_proj.bias', 'clip_model.transformer.resblocks.7.ln_1.weight', 'clip_model.transformer.resblocks.7.ln_1.bias', 'clip_model.transformer.resblocks.7.mlp.c_fc.weight', 'clip_model.transformer.resblocks.7.mlp.c_fc.bias', 'clip_model.transformer.resblocks.7.mlp.c_proj.weight', 'clip_model.transformer.resblocks.7.mlp.c_proj.bias', 'clip_model.transformer.resblocks.7.ln_2.weight', 'clip_model.transformer.resblocks.7.ln_2.bias', 'clip_model.transformer.resblocks.8.attn.in_proj_weight', 'clip_model.transformer.resblocks.8.attn.in_proj_bias', 'clip_model.transformer.resblocks.8.attn.out_proj.weight', 'clip_model.transformer.resblocks.8.attn.out_proj.bias', 'clip_model.transformer.resblocks.8.ln_1.weight', 'clip_model.transformer.resblocks.8.ln_1.bias', 'clip_model.transformer.resblocks.8.mlp.c_fc.weight', 'clip_model.transformer.resblocks.8.mlp.c_fc.bias', 'clip_model.transformer.resblocks.8.mlp.c_proj.weight', 'clip_model.transformer.resblocks.8.mlp.c_proj.bias', 'clip_model.transformer.resblocks.8.ln_2.weight', 'clip_model.transformer.resblocks.8.ln_2.bias', 'clip_model.transformer.resblocks.9.attn.in_proj_weight', 'clip_model.transformer.resblocks.9.attn.in_proj_bias', 'clip_model.transformer.resblocks.9.attn.out_proj.weight', 'clip_model.transformer.resblocks.9.attn.out_proj.bias', 'clip_model.transformer.resblocks.9.ln_1.weight', 'clip_model.transformer.resblocks.9.ln_1.bias', 'clip_model.transformer.resblocks.9.mlp.c_fc.weight', 'clip_model.transformer.resblocks.9.mlp.c_fc.bias', 'clip_model.transformer.resblocks.9.mlp.c_proj.weight', 'clip_model.transformer.resblocks.9.mlp.c_proj.bias', 'clip_model.transformer.resblocks.9.ln_2.weight', 'clip_model.transformer.resblocks.9.ln_2.bias', 'clip_model.transformer.resblocks.10.attn.in_proj_weight', 'clip_model.transformer.resblocks.10.attn.in_proj_bias', 'clip_model.transformer.resblocks.10.attn.out_proj.weight', 'clip_model.transformer.resblocks.10.attn.out_proj.bias', 'clip_model.transformer.resblocks.10.ln_1.weight', 'clip_model.transformer.resblocks.10.ln_1.bias', 'clip_model.transformer.resblocks.10.mlp.c_fc.weight', 'clip_model.transformer.resblocks.10.mlp.c_fc.bias', 'clip_model.transformer.resblocks.10.mlp.c_proj.weight', 'clip_model.transformer.resblocks.10.mlp.c_proj.bias', 'clip_model.transformer.resblocks.10.ln_2.weight', 'clip_model.transformer.resblocks.10.ln_2.bias', 'clip_model.transformer.resblocks.11.attn.in_proj_weight', 'clip_model.transformer.resblocks.11.attn.in_proj_bias', 'clip_model.transformer.resblocks.11.attn.out_proj.weight', 'clip_model.transformer.resblocks.11.attn.out_proj.bias', 'clip_model.transformer.resblocks.11.ln_1.weight', 'clip_model.transformer.resblocks.11.ln_1.bias', 'clip_model.transformer.resblocks.11.mlp.c_fc.weight', 'clip_model.transformer.resblocks.11.mlp.c_fc.bias', 'clip_model.transformer.resblocks.11.mlp.c_proj.weight', 'clip_model.transformer.resblocks.11.mlp.c_proj.bias', 'clip_model.transformer.resblocks.11.ln_2.weight', 'clip_model.transformer.resblocks.11.ln_2.bias', 'clip_model.token_embedding.weight', 'clip_model.ln_final.weight', 'clip_model.ln_final.bias', 'model.class_embedding', 'model.positional_embedding', 'model.proj', 'model.conv1.weight', 'model.ln_pre.weight', 'model.ln_pre.bias', 'model.transformer.resblocks.0.attn.in_proj_weight', 'model.transformer.resblocks.0.attn.in_proj_bias', 'model.transformer.resblocks.0.attn.out_proj.weight', 'model.transformer.resblocks.0.attn.out_proj.bias', 'model.transformer.resblocks.0.ln_1.weight', 'model.transformer.resblocks.0.ln_1.bias', 'model.transformer.resblocks.0.mlp.c_fc.weight', 'model.transformer.resblocks.0.mlp.c_fc.bias', 'model.transformer.resblocks.0.mlp.c_proj.weight', 'model.transformer.resblocks.0.mlp.c_proj.bias', 'model.transformer.resblocks.0.ln_2.weight', 'model.transformer.resblocks.0.ln_2.bias', 'model.transformer.resblocks.1.attn.in_proj_weight', 'model.transformer.resblocks.1.attn.in_proj_bias', 'model.transformer.resblocks.1.attn.out_proj.weight', 'model.transformer.resblocks.1.attn.out_proj.bias', 'model.transformer.resblocks.1.ln_1.weight', 'model.transformer.resblocks.1.ln_1.bias', 'model.transformer.resblocks.1.mlp.c_fc.weight', 'model.transformer.resblocks.1.mlp.c_fc.bias', 'model.transformer.resblocks.1.mlp.c_proj.weight', 'model.transformer.resblocks.1.mlp.c_proj.bias', 'model.transformer.resblocks.1.ln_2.weight', 'model.transformer.resblocks.1.ln_2.bias', 'model.transformer.resblocks.2.attn.in_proj_weight', 'model.transformer.resblocks.2.attn.in_proj_bias', 'model.transformer.resblocks.2.attn.out_proj.weight', 'model.transformer.resblocks.2.attn.out_proj.bias', 'model.transformer.resblocks.2.ln_1.weight', 'model.transformer.resblocks.2.ln_1.bias', 'model.transformer.resblocks.2.mlp.c_fc.weight', 'model.transformer.resblocks.2.mlp.c_fc.bias', 'model.transformer.resblocks.2.mlp.c_proj.weight', 'model.transformer.resblocks.2.mlp.c_proj.bias', 'model.transformer.resblocks.2.ln_2.weight', 'model.transformer.resblocks.2.ln_2.bias', 'model.transformer.resblocks.3.attn.in_proj_weight', 'model.transformer.resblocks.3.attn.in_proj_bias', 'model.transformer.resblocks.3.attn.out_proj.weight', 'model.transformer.resblocks.3.attn.out_proj.bias', 'model.transformer.resblocks.3.ln_1.weight', 'model.transformer.resblocks.3.ln_1.bias', 'model.transformer.resblocks.3.mlp.c_fc.weight', 'model.transformer.resblocks.3.mlp.c_fc.bias', 'model.transformer.resblocks.3.mlp.c_proj.weight', 'model.transformer.resblocks.3.mlp.c_proj.bias', 'model.transformer.resblocks.3.ln_2.weight', 'model.transformer.resblocks.3.ln_2.bias', 'model.transformer.resblocks.4.attn.in_proj_weight', 'model.transformer.resblocks.4.attn.in_proj_bias', 'model.transformer.resblocks.4.attn.out_proj.weight', 'model.transformer.resblocks.4.attn.out_proj.bias', 'model.transformer.resblocks.4.ln_1.weight', 'model.transformer.resblocks.4.ln_1.bias', 'model.transformer.resblocks.4.mlp.c_fc.weight', 'model.transformer.resblocks.4.mlp.c_fc.bias', 'model.transformer.resblocks.4.mlp.c_proj.weight', 'model.transformer.resblocks.4.mlp.c_proj.bias', 'model.transformer.resblocks.4.ln_2.weight', 'model.transformer.resblocks.4.ln_2.bias', 'model.transformer.resblocks.5.attn.in_proj_weight', 'model.transformer.resblocks.5.attn.in_proj_bias', 'model.transformer.resblocks.5.attn.out_proj.weight', 'model.transformer.resblocks.5.attn.out_proj.bias', 'model.transformer.resblocks.5.ln_1.weight', 'model.transformer.resblocks.5.ln_1.bias', 'model.transformer.resblocks.5.mlp.c_fc.weight', 'model.transformer.resblocks.5.mlp.c_fc.bias', 'model.transformer.resblocks.5.mlp.c_proj.weight', 'model.transformer.resblocks.5.mlp.c_proj.bias', 'model.transformer.resblocks.5.ln_2.weight', 'model.transformer.resblocks.5.ln_2.bias', 'model.transformer.resblocks.6.attn.in_proj_weight', 'model.transformer.resblocks.6.attn.in_proj_bias', 'model.transformer.resblocks.6.attn.out_proj.weight', 'model.transformer.resblocks.6.attn.out_proj.bias', 'model.transformer.resblocks.6.ln_1.weight', 'model.transformer.resblocks.6.ln_1.bias', 'model.transformer.resblocks.6.mlp.c_fc.weight', 'model.transformer.resblocks.6.mlp.c_fc.bias', 'model.transformer.resblocks.6.mlp.c_proj.weight', 'model.transformer.resblocks.6.mlp.c_proj.bias', 'model.transformer.resblocks.6.ln_2.weight', 'model.transformer.resblocks.6.ln_2.bias', 'model.transformer.resblocks.7.attn.in_proj_weight', 'model.transformer.resblocks.7.attn.in_proj_bias', 'model.transformer.resblocks.7.attn.out_proj.weight', 'model.transformer.resblocks.7.attn.out_proj.bias', 'model.transformer.resblocks.7.ln_1.weight', 'model.transformer.resblocks.7.ln_1.bias', 'model.transformer.resblocks.7.mlp.c_fc.weight', 'model.transformer.resblocks.7.mlp.c_fc.bias', 'model.transformer.resblocks.7.mlp.c_proj.weight', 'model.transformer.resblocks.7.mlp.c_proj.bias', 'model.transformer.resblocks.7.ln_2.weight', 'model.transformer.resblocks.7.ln_2.bias', 'model.transformer.resblocks.8.attn.in_proj_weight', 'model.transformer.resblocks.8.attn.in_proj_bias', 'model.transformer.resblocks.8.attn.out_proj.weight', 'model.transformer.resblocks.8.attn.out_proj.bias', 'model.transformer.resblocks.8.ln_1.weight', 'model.transformer.resblocks.8.ln_1.bias', 'model.transformer.resblocks.8.mlp.c_fc.weight', 'model.transformer.resblocks.8.mlp.c_fc.bias', 'model.transformer.resblocks.8.mlp.c_proj.weight', 'model.transformer.resblocks.8.mlp.c_proj.bias', 'model.transformer.resblocks.8.ln_2.weight', 'model.transformer.resblocks.8.ln_2.bias', 'model.transformer.resblocks.9.attn.in_proj_weight', 'model.transformer.resblocks.9.attn.in_proj_bias', 'model.transformer.resblocks.9.attn.out_proj.weight', 'model.transformer.resblocks.9.attn.out_proj.bias', 'model.transformer.resblocks.9.ln_1.weight', 'model.transformer.resblocks.9.ln_1.bias', 'model.transformer.resblocks.9.mlp.c_fc.weight', 'model.transformer.resblocks.9.mlp.c_fc.bias', 'model.transformer.resblocks.9.mlp.c_proj.weight', 'model.transformer.resblocks.9.mlp.c_proj.bias', 'model.transformer.resblocks.9.ln_2.weight', 'model.transformer.resblocks.9.ln_2.bias', 'model.transformer.resblocks.10.attn.in_proj_weight', 'model.transformer.resblocks.10.attn.in_proj_bias', 'model.transformer.resblocks.10.attn.out_proj.weight', 'model.transformer.resblocks.10.attn.out_proj.bias', 'model.transformer.resblocks.10.ln_1.weight', 'model.transformer.resblocks.10.ln_1.bias', 'model.transformer.resblocks.10.mlp.c_fc.weight', 'model.transformer.resblocks.10.mlp.c_fc.bias', 'model.transformer.resblocks.10.mlp.c_proj.weight', 'model.transformer.resblocks.10.mlp.c_proj.bias', 'model.transformer.resblocks.10.ln_2.weight', 'model.transformer.resblocks.10.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'model.ln_post.weight', 'model.ln_post.bias', 'reduce.weight', 'reduce.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load clipseg model\n",
    "model_clip = CLIPDensePredT(version=\"ViT-B/16\", reduce_dim=64)\n",
    "model_clip.eval()\n",
    "\n",
    "# non-strict, because we only stored decoder weights (not CLIP weights)\n",
    "model_clip.load_state_dict(\n",
    "    torch.load(\"clipseg/weights/rd64-uni.pth\", map_location=torch.device(\"cuda\")),\n",
    "    strict=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP SPECTRAL\n",
    "def get_model(name: str):\n",
    "    if \"dino\" in name:\n",
    "        model = torch.hub.load(\"facebookresearch/dino:main\", name)\n",
    "        model.fc = torch.nn.Identity()\n",
    "        val_transform = get_transform(name)\n",
    "        patch_size = model.patch_embed.patch_size\n",
    "        num_heads = model.blocks[0].attn.num_heads\n",
    "    elif name in [\"mocov3_vits16\", \"mocov3_vitb16\"]:\n",
    "        model = torch.hub.load(\n",
    "            \"facebookresearch/dino:main\", name.replace(\"mocov3\", \"dino\")\n",
    "        )\n",
    "        checkpoint_file, size_char = {\n",
    "            \"mocov3_vits16\": (\"vit-s-300ep-timm-format.pth\", \"s\"),\n",
    "            \"mocov3_vitb16\": (\"vit-b-300ep-timm-format.pth\", \"b\"),\n",
    "        }[name]\n",
    "        url = f\"https://dl.fbaipublicfiles.com/moco-v3/vit-{size_char}-300ep/vit-{size_char}-300ep.pth.tar\"\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        model.fc = torch.nn.Identity()\n",
    "        val_transform = get_transform(name)\n",
    "        patch_size = model.patch_embed.patch_size\n",
    "        num_heads = model.blocks[0].attn.num_heads\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {name}\")\n",
    "    model = model.eval()\n",
    "    return model, val_transform, patch_size, num_heads\n",
    "\n",
    "\n",
    "def get_transform(name: str):\n",
    "    if any(\n",
    "        x in name\n",
    "        for x in (\n",
    "            \"dino\",\n",
    "            \"mocov3\",\n",
    "            \"convnext\",\n",
    "        )\n",
    "    ):\n",
    "        normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    size=512, interpolation=TF.InterpolationMode.BICUBIC, max_size=1024\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_diagonal(W: scipy.sparse.csr_matrix, threshold: float = 1e-12):\n",
    "    D = W.dot(np.ones(W.shape[1], W.dtype))\n",
    "    D[D < threshold] = 1.0  # Prevent division by zero.\n",
    "    D = scipy.sparse.diags(D)\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\yitao/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available, using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Parameters\n",
    "model_name = \"dino_vitb16\"  # TODO: Figure out how to make this user-editable\n",
    "K = 5\n",
    "\n",
    "# Load model\n",
    "model_dss, val_transform, patch_size, num_heads = get_model(model_name)\n",
    "\n",
    "# Add hook\n",
    "which_block = -1\n",
    "if \"dino\" in model_name or \"mocov3\" in model_name:\n",
    "    feat_out = {}\n",
    "\n",
    "    def hook_fn_forward_qkv(module, input, output):\n",
    "        feat_out[\"qkv\"] = output\n",
    "\n",
    "    handle: RemovableHandle = (\n",
    "        model_dss._modules[\"blocks\"][which_block]\n",
    "        ._modules[\"attn\"]\n",
    "        ._modules[\"qkv\"]\n",
    "        .register_forward_hook(hook_fn_forward_qkv)\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(model_name)\n",
    "\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available, using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_dss.to(device)\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def segment(inp: Image):\n",
    "    # NOTE: The image is already resized to the desired size.\n",
    "\n",
    "    # Preprocess image\n",
    "    images: torch.Tensor = val_transform(inp)\n",
    "    images = images.unsqueeze(0).to(device)\n",
    "\n",
    "    # Reshape image\n",
    "    P = patch_size\n",
    "    B, C, H, W = images.shape\n",
    "    H_patch, W_patch = H // P, W // P\n",
    "    H_pad, W_pad = H_patch * P, W_patch * P\n",
    "    T = H_patch * W_patch + 1  # number of tokens, add 1 for [CLS]\n",
    "\n",
    "    # Crop image to be a multiple of the patch size\n",
    "    images = images[:, :, :H_pad, :W_pad]\n",
    "\n",
    "    # Extract features\n",
    "    if \"dino\" in model_name or \"mocov3\" in model_name:\n",
    "        model_dss.get_intermediate_layers(images)[0].squeeze(0)\n",
    "        output_qkv = (\n",
    "            feat_out[\"qkv\"]\n",
    "            .reshape(B, T, 3, num_heads, -1 // num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        feats = output_qkv[1].transpose(1, 2).reshape(B, T, -1)[:, 1:, :].squeeze(0)\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "\n",
    "    # Normalize features\n",
    "    normalize = True\n",
    "    if normalize:\n",
    "        feats = F.normalize(feats, p=2, dim=-1)\n",
    "\n",
    "    # Compute affinity matrix\n",
    "    W_feat = feats @ feats.T\n",
    "\n",
    "    # Feature affinities\n",
    "    threshold_at_zero = True\n",
    "    if threshold_at_zero:\n",
    "        W_feat = W_feat * (W_feat > 0)\n",
    "    W_feat = (\n",
    "        W_feat / W_feat.max()\n",
    "    )  # NOTE: If features are normalized, this naturally does nothing\n",
    "    W_feat = W_feat.cpu().numpy()\n",
    "\n",
    "    # # NOTE: Here is where we would add the color information. For simplicity, we will not add it here.\n",
    "    # W_comb = W_feat + W_color * image_color_lambda  # combination\n",
    "    # D_comb = np.array(get_diagonal(W_comb).todense())  # is dense or sparse faster? not sure, should check\n",
    "\n",
    "    # Diagonal\n",
    "    W_comb = W_feat\n",
    "    D_comb = np.array(\n",
    "        get_diagonal(W_comb).todense()\n",
    "    )  # is dense or sparse faster? not sure, should check\n",
    "\n",
    "    # Compute eigenvectors\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = eigsh(\n",
    "            D_comb - W_comb, k=(K + 1), sigma=0, which=\"LM\", M=D_comb\n",
    "        )\n",
    "    except:\n",
    "        eigenvalues, eigenvectors = eigsh(\n",
    "            D_comb - W_comb, k=(K + 1), which=\"SM\", M=D_comb\n",
    "        )\n",
    "    eigenvalues = torch.from_numpy(eigenvalues)\n",
    "    eigenvectors = torch.from_numpy(eigenvectors.T).float()\n",
    "\n",
    "    # Resolve sign ambiguity\n",
    "    for k in range(eigenvectors.shape[0]):\n",
    "        if (\n",
    "            0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0\n",
    "        ):  # reverse segment\n",
    "            eigenvectors[k] = 0 - eigenvectors[k]\n",
    "\n",
    "    # Arrange eigenvectors into grid\n",
    "    # cmap = get_cmap('viridis')\n",
    "    output_images = []\n",
    "    # eigenvectors_upscaled = []\n",
    "    for i in range(1, K + 1):\n",
    "        eigenvector = eigenvectors[i].reshape(\n",
    "            1, 1, H_patch, W_patch\n",
    "        )  # .reshape(1, 1, H_pad, W_pad)\n",
    "        eigenvector: torch.Tensor = F.interpolate(\n",
    "            eigenvector, size=(H_pad, W_pad), mode=\"bicubic\", align_corners=False\n",
    "        )  # slightly off, but for visualizations this is okay\n",
    "        buffer = io.BytesIO()\n",
    "        plt.imsave(\n",
    "            buffer, eigenvector.squeeze().numpy(), format=\"png\"\n",
    "        )  # save to a temporary location\n",
    "        buffer.seek(0)\n",
    "        eigenvector_vis = Image.open(buffer).convert(\"RGB\")\n",
    "        # eigenvector_vis = TF.to_tensor(eigenvector_vis).unsqueeze(0)\n",
    "        eigenvector_vis = np.array(eigenvector_vis)\n",
    "        # eigenvectors_upscaled.append(eigenvector)\n",
    "        output_images.append(eigenvector_vis)\n",
    "    # output_images = torch.cat(output_images, dim=0)\n",
    "    # output_images = make_grid(output_images, nrow=8, pad_value=1)\n",
    "\n",
    "    # Also add CRF\n",
    "    if False:\n",
    "        # Imports\n",
    "        import denseCRF\n",
    "\n",
    "        # Parameters\n",
    "        ParamsCRF = namedtuple(\"ParamsCRF\", \"w1 alpha beta w2 gamma it\")\n",
    "        DEFAULT_CRF_PARAMS = ParamsCRF(\n",
    "            w1=6,  # weight of bilateral term  # 10.0,\n",
    "            alpha=40,  # spatial std  # 80,\n",
    "            beta=13,  # rgb  std  # 13,\n",
    "            w2=3,  # weight of spatial term  # 3.0,\n",
    "            gamma=3,  # spatial std  # 3,\n",
    "            it=5.0,  # iteration  # 5.0,\n",
    "        )\n",
    "\n",
    "        # Get unary potentials\n",
    "        unary_potentials = eigenvectors_upscaled[0].squeeze(1).squeeze(0)\n",
    "        unary_potentials = (unary_potentials - unary_potentials.min()) / (\n",
    "            unary_potentials.max() - unary_potentials.min()\n",
    "        )\n",
    "        unary_potentials_np = (\n",
    "            torch.stack((1 - unary_potentials, unary_potentials), dim=-1).cpu().numpy()\n",
    "        )\n",
    "        img_np = images.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        img_np = (img_np * 255).astype(np.uint8)[0]\n",
    "\n",
    "        # Return result of CRF\n",
    "        out = denseCRF.densecrf(img_np, unary_potentials_np, DEFAULT_CRF_PARAMS)\n",
    "        out = out * 255\n",
    "        output_images.append(out)\n",
    "\n",
    "    # # Postprocess for Gradio\n",
    "    # output_images = np.array(TF.to_pil_image(output_images))\n",
    "    print(f\"{len(output_images)=}\")\n",
    "\n",
    "    # Garbage collection and other memory-related things\n",
    "    gc.collect()\n",
    "    del eigenvector, eigenvector_vis, eigenvectors, W_comb, D_comb\n",
    "\n",
    "    return output_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
